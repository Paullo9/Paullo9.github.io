{"version":"1","records":[{"hierarchy":{"lvl1":"How to MyST, without being mystified üßô"},"type":"lvl1","url":"/paper","position":0},{"hierarchy":{"lvl1":"How to MyST, without being mystified üßô"},"content":"A tutorial to evolve markdown documents and notebooks into structured data\n\nAuthors: Rowan Cockett1,2 Affiliations: 1Executable Books, 2 Curvenote License: CC-BY\n\nAbstract\n\nWe introduce, a set of open-source, community-driven tools for MyST Markdown (\n\nmyst.tools) designed for scientific communication, including a powerful authoring framework that supports blogs, online books, scientific papers, reports and journals articles.","type":"content","url":"/paper","position":1},{"hierarchy":{"lvl1":"How to MyST, without being mystified üßô","lvl2":"Background"},"type":"lvl2","url":"/paper#background","position":2},{"hierarchy":{"lvl1":"How to MyST, without being mystified üßô","lvl2":"Background"},"content":"Scientific communication today is designed around print documents and pay-walled access to content. Over the last decade, the open-science movement has accelerated the use of pre-print services and data archives that are vastly improving the accessibility of scientific content. However, these systems are not designed for communicating modern scientific outputs, which encompasses so much more than a paper-centric model of the scholarly literature.\n\nWe believe how we share and communicate scientific knowledge should evolve past the status quo of print-based publishing and all the limitations of paper.\n\nThe communication and collaboration tools that we are building in the Project Jupyter are built to follow the FORCE11 recommendations (Bourne et al., 2012). Specifically:\n\nrethink the unit and form of scholarly publication;\n\ndevelop tools and technologies to better support the scholarly lifecycle; and\n\nadd data, software, and workflows as first-class research objects.\n\nBy bringing professional, high-quality tools for science communication into the research lifecycle, we believe we can improve the collection and preservation of scholarly metadata (citations, cross-references, annotations, etc.) as well as open up new ways to communicate science with interactive figures & equations, computation, and reactivity.\n\nThe tools that are being built by the Project Jupyter are focused on introducing a new Markup language, MyST (Markedly Structured Text), that works seamlessly with the Jupyter community to enhance and promote a new path to document creation and publishing for next-generation scientific textbooks, blogs, and lectures. Our team is currently supported by the \n\nSloan Foundation, (\n\nGrant #9231).\n\nMyST enables rich content generation and is a powerful format for scientific and technical communication. JupyterBook uses MyST and has broad adoption in publishing tutorials and educational content focused around Jupyter Notebooks.\n\nThe components behind Jupyter Book are downloaded 30,000 times a day, with 750K downloads last month.\n\nThe current toolchain used by \n\nJupyterBook is based on \n\nSphinx, which is an open-source documentation system used in many software projects, especially in the Python ecosystem. mystjs is a similar tool to \n\nSphinx, however, designed specifically for scientific communication. In addition to building websites, mystjs can also help you create scientific PDFs, Microsoft Word documents, and JATS XML (used in scientific publishing).\n\nmystjs uses existing, modern web-frameworks in place of the \n\nSphinx build system. These tools come out-of-the-box with prefetching for faster navigation, smaller network payloads through modern web-bundlers, image optimization, partial-page refresh through single-page application. Many of these features, performance and accessibility improvements are difficult, if not impossible, to create inside of the \n\nSphinx build system.\n\nIn 2022, the Executable Books team started work to document the specification behind the markup language, called \n\nmyst-spec, this work has enabled other tools and implementations in the scientific ecosystem to build on MyST (e.g. \n\nscientific authoring tools, and \n\ndocumentation systems).\n\nThe mystjs ecosystem was developed as a collaboration between \n\nCurvenote, \n\n2i2c and the \n\nExecutableBooks team. The initial version of mystjs was originally release by \n\nCurvenote as the \n\nCurvenote CLI under the MIT license, and transferred to the \n\nExecutableBooks team in October 2022. The goal of the project is to enable the same rich content and authoring experiences that \n\nSphinx allows for software documentation, with a focus on web-first technologies (Javascript), interactivity, accessibility, scientific references (e.g. DOIs and other persistent IDs), professional PDF outputs, and JATS XML documents for scientific archiving.","type":"content","url":"/paper#background","position":3},{"hierarchy":{"lvl1":"How to MyST, without being mystified üßô","lvl2":"MyST Project"},"type":"lvl2","url":"/paper#myst-project","position":4},{"hierarchy":{"lvl1":"How to MyST, without being mystified üßô","lvl2":"MyST Project"},"content":"In this paper we introduce mystjs, which allows the popular MyST Markdown syntax to be run directly in a web browser, opening up new workflows for components to be used in web-based editors, \n\ndirectly in Jupyter and in JupyterLite. The libraries work with current MyST Markdown documents/projects and can export to \n\nLaTeX/PDF, \n\nMicrosoft Word and \n\nJATS as well as multiple website templates using a \n\nmodern React-based renderer. There are currently over 400 scientific journals that are supported through \n\ntemplates, with \n\nnew LaTeX templates that can be added easily using a Jinja-based templating package, called \n\njtex.\n\nIn our paper we will give an overview of the MyST ecosystem, how to use MyST tools in conjunction with existing Jupyter Notebooks, markdown documents, and JupyterBooks to create professional PDFs and interactive websites, books, blogs and scientific articles. We give special attention to the additions around structured data, standards in publishing (e.g. efforts in representing Notebooks as JATS XML), rich \n\nfrontmatter and bringing \n\ncross-references and \n\npersistent IDs to life with interactive hover-tooltips (\n\nORCID, RoR, \n\nRRIDs, \n\nDOIs, \n\nintersphinx, \n\nwikipedia, \n\nJATS, \n\nGitHub code, and more!). This rich metadata and structured content can be used directly to improve science communication both through self-publishing books, blogs, and lab websites ‚Äî as well as journals that incorporate Jupyter Notebooks.","type":"content","url":"/paper#myst-project","position":5},{"hierarchy":{"lvl1":"How to MyST, without being mystified üßô","lvl2":"Features of MyST"},"type":"lvl2","url":"/paper#features-of-myst","position":6},{"hierarchy":{"lvl1":"How to MyST, without being mystified üßô","lvl2":"Features of MyST"},"content":"MyST is focused on scientific writing, and ensuring that citations are first class both for writing and for reading (see Figure 1).\n\n\nFigure 1: Citations are rendered with a popup directly inline.\n\nMyST aims to show as much information in context as possible, for example, Figure 2 shows a reading experience for a referenced equation: you can immediately click on the reference, see the equation, all without loosing any context -- ultimately saving you time. Head et al. (2021) found that these ideas both improved the overall reading experience of articles as well as allowed researchers to answer questions about an article 26% faster when compared to a traditional PDF!\n\n\nFigure 2: In context cross-references improve the reading experience.\n\nOne of the important underlying goals of practicing reproducibility, sharing more of the methods and data behind a scientific work so that other researchers can both verify as well as build upon your findings. One of the exciting ways to pull for reproducibility is to make documents directly linked to data and computation! In Figure 3, we are showing outputs from a Jupyter Notebook directly part of the published scientific narrative.\n\n\nFigure 3: Embedding data, interactivity and computation into a MyST article.\n\nTo drive all of these features, the contents of a MyST document needs to be well defined. This is critical for powering interactive hovers, linked citations, and compatibility with scientific publishing standards like the Journal Article Metadata Tag Suite (JATS). We have an emerging specification for MyST, \n\nmyst-spec, that aims to capture this information and transform it between many different formats, like PDF, Word, JSON, and JATS XML (Figure 4). This specification is arrived at through a community-centric MyST Enhancement Proposal (\n\nMEP) process.\n\n\nFigure 4: The data behind MyST is structured, which means we can transform it into many different document types and use it to power all sorts of exciting features!\n\nOne of the common forms of scientific communication today is through PDF documents. MyST has excellent support for creating PDF documents, using a data-driven templating library called jtex. The document in Figure 5 was created using MyST!\n\n\nFigure 5: A PDF rendering through MyST.","type":"content","url":"/paper#features-of-myst","position":7},{"hierarchy":{"lvl1":"How to MyST, without being mystified üßô","lvl2":"Conclusion"},"type":"lvl2","url":"/paper#conclusion","position":8},{"hierarchy":{"lvl1":"How to MyST, without being mystified üßô","lvl2":"Conclusion"},"content":"There are many opportunities to improve open-science communication, to make it more interactive, accessible, more reproducible, and both produce and use structured data throughout the research-writing process. The mystjs ecosystem of tools is designed with structured data at its core. We would love if you gave it a try -- learn to get started at \n\nhttps://myst.tools.","type":"content","url":"/paper#conclusion","position":9},{"hierarchy":{"lvl1":"How to MyST, without being mystified üßô","lvl2":"References"},"type":"lvl2","url":"/paper#references","position":10},{"hierarchy":{"lvl1":"How to MyST, without being mystified üßô","lvl2":"References"},"content":"Bourne, Philip E., Clark, Timothy W., Dale, Robert, De Waard, Anita, Herman, Ivan, Hovy, Eduard H., Shotton, David. (2012)‚ÄúImproving The Future of Research Communications and e-Scholarship‚Äù. FORCE11. doi:10.4230/DAGMAN.1.1.41\n\nHead, A., Lo, K., Kang, D., Fok, R., Skjonsberg, S., Weld, D. S., & Hearst, M. A. (2021, May). Augmenting Scientific Papers with Just-in-Time, Position-Sensitive Definitions of Terms and Symbols. Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 10.1145/3411764.3445648","type":"content","url":"/paper#references","position":11},{"hierarchy":{"lvl1":"Linking Interactive Notebooks"},"type":"lvl1","url":"/notebook","position":0},{"hierarchy":{"lvl1":"Linking Interactive Notebooks"},"content":"MyST allows you to directly include Jupyter Notebooks in your books, documents and websites.\nThis Jupyter Notebook can be rendered directly using MyST.\n\nFor example, let us import altair and create a demo of an interactive plot!\n\nimport altair as alt\nfrom vega_datasets import data\n\nsource = data.cars()\nbrush = alt.selection_interval(encodings=['x'])\npoints = alt.Chart(source).mark_point().encode(\n    x='Horsepower:Q',\n    y='Miles_per_Gallon:Q',\n    size='Acceleration',\n    color=alt.condition(brush, 'Origin:N', alt.value('lightgray'))\n).add_selection(brush)\n\nbars = alt.Chart(source).mark_bar().encode(\n    y='Origin:N',\n    color='Origin:N',\n    x='count(Origin):Q'\n).transform_filter(brush)\n\nWe can now plot the altair example, which is fully interactive, try dragging in the plot to select cars by their horsepower.\n\npoints & bars\n\n# https://matplotlib.org/stable/gallery/statistics/time_series_histogram.html#sphx-glr-gallery-statistics-time-series-histogram-py\nfrom copy import copy\n\nimport numpy as np\nimport numpy.matlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LogNorm\n\n# Make some data; a 1D random walk + small fraction of sine waves\nnum_series = 1000\nnum_points = 100\nSNR = 0.10  # Signal to Noise Ratio\nx = np.linspace(0, 4 * np.pi, num_points)\n# Generate unbiased Gaussian random walks\nY = np.cumsum(np.random.randn(num_series, num_points), axis=-1)\n# Generate sinusoidal signals\nnum_signal = int(round(SNR * num_series))\nphi = (np.pi / 8) * np.random.randn(num_signal, 1)  # small random offset\nY[-num_signal:] = (\n    np.sqrt(np.arange(num_points))[None, :]  # random walk RMS scaling factor\n    * (np.sin(x[None, :] - phi)\n       + 0.05 * np.random.randn(num_signal, num_points))  # small random noise\n)\n\n\n# Now we will convert the multiple time series into a histogram. Not only will\n# the hidden signal be more visible, but it is also a much quicker procedure.\n# Linearly interpolate between the points in each time series\nnum_fine = 800\nx_fine = np.linspace(x.min(), x.max(), num_fine)\ny_fine = np.empty((num_series, num_fine), dtype=float)\nfor i in range(num_series):\n    y_fine[i, :] = np.interp(x_fine, x, Y[i, :])\ny_fine = y_fine.flatten()\nx_fine = np.matlib.repmat(x_fine, num_series, 1).flatten()\n\nImportant!\nThis data is simulated, and may just be random noise! üîä\n\nfig, axes = plt.subplots(figsize=(8, 4), constrained_layout=True)\ncmap = copy(plt.cm.plasma)\ncmap.set_bad(cmap(0))\nh, xedges, yedges = np.histogram2d(x_fine, y_fine, bins=[400, 100])\npcm = axes.pcolormesh(xedges, yedges, h.T, cmap=cmap,\n                         norm=LogNorm(vmax=1.5e2), rasterized=True)\nfig.colorbar(pcm, ax=axes, label=\"# points\", pad=0)\naxes.set_title(\"2d histogram and log color scale\");\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\ndt = 0.01\nt = np.arange(0, 30, dt)\nnse1 = np.random.randn(len(t))                 # white noise 1\nnse2 = np.random.randn(len(t))                 # white noise 2\n\n# Two signals with a coherent part at 10 Hz and a random part\ns1 = np.sin(2 * np.pi * 10 * t) + nse1\ns2 = np.sin(2 * np.pi * 10 * t) + nse2\n\nfig, axs = plt.subplots(2, 1, layout='constrained')\naxs[0].plot(t, s1, t, s2)\naxs[0].set_xlim(0, 2)\naxs[0].set_xlabel('Time (s)')\naxs[0].set_ylabel('s1 and s2')\naxs[0].grid(True)\n\ncxy, f = axs[1].cohere(s1, s2, 256, 1. / dt)\naxs[1].set_ylabel('Coherence')\n\nplt.show()","type":"content","url":"/notebook","position":1},{"hierarchy":{"lvl1":"MyST Quickstart"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"MyST Quickstart"},"content":"\n\nThis repository contains the files used in the \n\nquickstart guide, and can be used to follow that guide, before trying MyST with your own content.\n\nNote\n\n This is not a good example of an actual MyST project! The repositories purpose is to be a simple markdown + notebook repository that can be transformed throughout a tutorial.\n\nThe goals of the \n\nquickstart guide are:\n\nCreate a myst site, using the standard template\n\nImprove the frontmatter, to add authors, affiliations and other metadata\n\nExport the paper as a PDF, Word document, and LaTeX files\n\nIntegrate a Jupyter Notebook output into our paper, to improve reproducibility\n\nPublish a website of with our work üöÄ","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"MyST Quickstart","lvl2":"Improving Frontmatter and MyST Site"},"type":"lvl2","url":"/#improving-frontmatter-and-myst-site","position":2},{"hierarchy":{"lvl1":"MyST Quickstart","lvl2":"Improving Frontmatter and MyST Site"},"content":"","type":"content","url":"/#improving-frontmatter-and-myst-site","position":3},{"hierarchy":{"lvl1":"MyST Quickstart","lvl2":"Export as a PDF"},"type":"lvl2","url":"/#export-as-a-pdf","position":4},{"hierarchy":{"lvl1":"MyST Quickstart","lvl2":"Export as a PDF"},"content":"","type":"content","url":"/#export-as-a-pdf","position":5},{"hierarchy":{"lvl1":"Home"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Home"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Home","lvl2":"Welcome"},"type":"lvl2","url":"/#welcome","position":2},{"hierarchy":{"lvl1":"Home","lvl2":"Welcome"},"content":"Welcome to the website for my master‚Äôs thesis on 4D scanning transmission electron microscopy (4D-STEM), an advanced technique in transmission electron microscopy setup. In 4D-STEM, a convergent electron beam is scanned across a sample, and a complete 2D electron diffraction pattern is recorded for each probe position. This generates a 4-dimensional dataset enabling sophisticated material analysis, such as virtual dark and bright-field imaging, crystal orientation analysis, and strain mapping.\n\nHandling and processing these extensive datasets, often several hundred gigabytes in size, is a complex task. To manage this, we will utilize the OMNI Computing Cluster (HPC) at the University of Siegen. To transform the data into into interpretable formats we will be using the py4DSTEM library, it supports diverse modes of 4D-STEM analysis for both traditional and advanced imaging techniques.\n\nThis website will document the setup of the computational environment, literature reviews, and progress updates related to my thesis. The primary files associated with my thesis will be maintained/compiled on Overleaf, while my GitHub repository will serve as the version control system for the Latex files (.tex) and the Jupyter notebooks (.ipynb), which integrate python code and markdown for clarity and ease of collaboration.","type":"content","url":"/#welcome","position":3},{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/introduction","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"","type":"content","url":"/introduction","position":1},{"hierarchy":{"lvl1":"Load and examine a dataset"},"type":"lvl1","url":"/first-steps","position":0},{"hierarchy":{"lvl1":"Load and examine a dataset"},"content":"sample by HW, mesurment by CO\n\nhigh-pressure torsion sample, Ni65Cu35\n\n","type":"content","url":"/first-steps","position":1},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Set environment: Load the packages"},"type":"lvl2","url":"/first-steps#set-environment-load-the-packages","position":2},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Set environment: Load the packages"},"content":"for details on how to connect to the cluster and set up environments visit \n\npaullobpreis.com (pw:4DSTEM)\n\n# timestr used to create simple timestamps for easier version controll\nimport time\ntimestr = time.strftime(\"%Y%m%d\")\n\n# py4dstem as main tool, the follwing command also prints the currently used version\nimport py4DSTEM\npy4DSTEM.__version__\n\n","type":"content","url":"/first-steps#set-environment-load-the-packages","position":3},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Load and examine a dataset"},"type":"lvl2","url":"/first-steps#load-and-examine-a-dataset","position":4},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Load and examine a dataset"},"content":"\n\n# Load the .dm4 file from a workspace on OMNI or XNAS, set filepath with 'filepath_data = \"FILEPATH\"'\n# even better, use dirpath'' and 'filepath_data', this makes it easier to add more paths later\n\ndirpath = \"/Users/paullobpreis/GitHub/Paullo9.github.io/data/\"\n\nfilepath_data = dirpath + 'COPL_Ni65Cu35_C_ROI3_240827_aper_50_conv_1.5_spot_6_CL_47_stepsize_10_r_x_178_r_y_186_GIF_512x512_preprocessed_unfiltered_bin_4_20240828.h5'\n\n# Load the datacube from the .dm4 file specified above\n\n# py4DSTEM uses the import_file function to load non-native file formats, while the read function is used for files originally written by py4DSTEM.\n\ndatacube = py4DSTEM.import_file(\nfilepath_data,\n)\n\n# datacube directly passed reveiles the 4-dimensional array of Real Space and Diffaction Space for the sample\n\ndatacube\n\n# The data itself can be observed by the following command:\n\ndatacube.data\n\n","type":"content","url":"/first-steps#load-and-examine-a-dataset","position":5},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Bin-on-load"},"type":"lvl2","url":"/first-steps#bin-on-load","position":6},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Bin-on-load"},"content":"Binnig is an option for large datasets that might not fit the computers RAM, hopefully we can skip this part as we are using the OMNI cluster.\n\n# this would turn a 128x128 dataset into a 64x64 dataset, hence 8 times smaller\n\ndatacube_binned = py4DSTEM.read(\n   filepath_data,\n   binfactor = 2\n)\n\ndatacube_binned\n\n","type":"content","url":"/first-steps#bin-on-load","position":7},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Calibrate pixel size and unit"},"type":"lvl2","url":"/first-steps#calibrate-pixel-size-and-unit","position":8},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Calibrate pixel size and unit"},"content":"\n\n# several properties of our datacube can be shown by:\nprint(datacube.data.shape)\nprint(datacube.shape)\nprint(datacube.Rshape)\nprint(datacube.Qshape)\n\n# Vectors calibrating each dimension of the dataset are included within the datacube, utilizing any available calibrations retrieved from the file\n\n# dimension vectors -\nprint('The first dimension:')\nprint(f'  - dimension name: {datacube.dim_names[0]}')\nprint(f'  - dimension units: {datacube.dim_units[0]}')\nprint(f'  - dim vector: {datacube.dims[0][:10]}') # note the `[:10]` - we're only displaying the first 10 entries\nprint()\nprint('The third dimension:')\nprint(f'  - dimension name: {datacube.dim_names[2]}')\nprint(f'  - dimension units: {datacube.dim_units[2]}')\nprint(f'  - dim vector: {datacube.dims[2][:10]}')\n\nprint()\n\n# pixel sizes -\nqpix = datacube.calibration.get_Q_pixel_size()\nqpixunit = datacube.calibration.get_Q_pixel_units()\nrpix = datacube.calibration.get_R_pixel_size()\nrpixunit = datacube.calibration.get_R_pixel_units()\nprint()\nprint(f\"The diffraction space pixels are each {qpix:.4f} {qpixunit}\")\nprint(f\"The real space pixels are each {rpix:.4f} {rpixunit}\")\n\n# complete list of calibrations is located here; the above vectors are derived from these values\n\ndatacube.calibration\n\n# Currently, the real space pixel size is shown as 1 pixel, indicating that this information was either unavailable or not extracted from the .dm4 file.\n# Assuming we know the real space pixel size between beam positions is 5 nanometers, we can update the value using:\n\ndatacube.calibration.set_R_pixel_size(5.1)\ndatacube.calibration.set_R_pixel_units('nm')\n\n# and print the newly calibrated values with:\ndatacube.calibration\n\n# the values will be automatically updated in the datacube\n\n","type":"content","url":"/first-steps#calibrate-pixel-size-and-unit","position":9},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Filterung and removing hot pixels, probably not needed"},"type":"lvl2","url":"/first-steps#filterung-and-removing-hot-pixels-probably-not-needed","position":10},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Filterung and removing hot pixels, probably not needed"},"content":"\n\n# The function works by finding pixels in the mean diffraction image that are thresh times brighter than any other pixel in their local neighborhood.\n# It then replaces these hot pixels in each diffraction pattern with the local median intensity.\n\ndatacube.filter_hot_pixels(\n    thresh = 8\n)\n\n\n","type":"content","url":"/first-steps#filterung-and-removing-hot-pixels-probably-not-needed","position":11},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Binning in diffraction space"},"type":"lvl2","url":"/first-steps#binning-in-diffraction-space","position":12},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Binning in diffraction space"},"content":"Loading unbinned data and bin it later is prefered, here is a function to do so, the binning factor is given in parentheses\n\ndatacube.bin_Q(4)\n\n","type":"content","url":"/first-steps#binning-in-diffraction-space","position":13},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Save the modified data"},"type":"lvl2","url":"/first-steps#save-the-modified-data","position":14},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Save the modified data"},"content":"Now we set a new filepath consisting of the old path with additions indicating our processing\n\nfrom the name alone should be clear if we preprosessed/filtered/binned\n\nadditionalls I implemented timestr to include the date\n\n# this data is saved as a .h5 file format \n\nfrom os.path import splitext\nfilepath_save = splitext(filepath_data)[0] + '_preprocessed_unfiltered_bin_4_' + timestr + '.h5'\n\n\n# print the new filepath \n\nprint(filepath_save)\n\n# Save\n\npy4DSTEM.save(\n    filepath_save,\n    datacube,\n    mode = 'o'    # 'overwrite' mode\n)\n\n","type":"content","url":"/first-steps#save-the-modified-data","position":15},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Inspect the resulting HDF5 file"},"type":"lvl2","url":"/first-steps#inspect-the-resulting-hdf5-file","position":16},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Inspect the resulting HDF5 file"},"content":"first we want to see where the data lives without opening it\n\nsecondly we check which default name was assigned\n\n# 'dm_dataset' and 'dm_dataset_root' are placeholders we could re-assigning a different name later\n\npy4DSTEM.print_h5_tree(filepath_save)\n\ndatacube.name\n\n","type":"content","url":"/first-steps#inspect-the-resulting-hdf5-file","position":17},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Note"},"type":"lvl2","url":"/first-steps#note","position":18},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Note"},"content":"When deciding whether or not to save a datacube to storage, it‚Äôs important to be mindful of the necessity. Since datacubes are typically large and accessible from the original microscope file, avoid creating new files unless there is a compelling reason.","type":"content","url":"/first-steps#note","position":19},{"hierarchy":{"lvl1":"Load and examine a dataset"},"type":"lvl1","url":"/loadandpre","position":0},{"hierarchy":{"lvl1":"Load and examine a dataset"},"content":"sample by HW, mesurment by CO\n\nhigh-pressure torsion sample, Ni65Cu35\n\n","type":"content","url":"/loadandpre","position":1},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl3":"Acknowledgements"},"type":"lvl3","url":"/loadandpre#acknowledgements","position":2},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl3":"Acknowledgements"},"content":"The following was created on basis of the turorial provided by the py4DSTEM instructor team:\n\nBen Savitzky (\n\nbhsavitzky@lbl.gov)\n\nSteve Zeltmann (\n\nsteven‚Äã.zeltmann@berkeley‚Äã.edu)\n\nStephanie Ribet (\n\nsribet@u‚Äã.northwestern‚Äã.edu)\n\nAlex Rakowski (\n\narakowski@lbl.gov)\n\nColin Ophus (\n\nclophus@lbl.gov)\n\n","type":"content","url":"/loadandpre#acknowledgements","position":3},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Set environment: Load the packages"},"type":"lvl2","url":"/loadandpre#set-environment-load-the-packages","position":4},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Set environment: Load the packages"},"content":"for details on how to connect to the cluster and set up environments visit \n\npaullobpreis.com (pw:4DSTEM)\n\n# timestr used to create simple timestamps for easier version controll\nimport time\ntimestr = time.strftime(\"%Y%m%d\")\n\n# py4dstem as main tool, the follwing command also prints the currently used version\nimport py4DSTEM\npy4DSTEM.__version__\n\n","type":"content","url":"/loadandpre#set-environment-load-the-packages","position":5},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Load the dataset"},"type":"lvl2","url":"/loadandpre#load-the-dataset","position":6},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Load the dataset"},"content":"\n\n# Load the .dm4 file from a workspace on OMNI or XNAS, set filepath with 'filepath_data = \"FILEPATH\"'\n# even better, use dirpath'' and 'filepath_data', this makes it easier to add more paths later\n\ndirpath = \"/fast/ws-tmp/g031362-data/high_pressure_torsion/lamC/ROI3/\"\n\n\nfilepath_data = dirpath + 'COPL_Ni65Cu35_C_ROI3_240827_aper_50_conv_1.5_spot_6_CL_47_stepsize_10_r_x_178_r_y_186_GIF_512x512.dm4'\n\n# Load the datacube from the .dm4 file specified above\n\n# py4DSTEM uses the import_file function to load non-native file formats, while the read function is used for files originally written by py4DSTEM.\n\ndatacube = py4DSTEM.import_file(\nfilepath_data,\n)\n\n# datacube directly passed reveiles the 4-dimensional array of Real Space and Diffaction Space for the sample\n\ndatacube\n\n","type":"content","url":"/loadandpre#load-the-dataset","position":7},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Calibrate pixel size and unit"},"type":"lvl2","url":"/loadandpre#calibrate-pixel-size-and-unit","position":8},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Calibrate pixel size and unit"},"content":"\n\n# several properties of our datacube can be shown by:\nprint(datacube.data.shape)\nprint(datacube.shape)\nprint(datacube.Rshape)\nprint(datacube.Qshape)\n\n# Vectors calibrating each dimension of the dataset are included within the datacube, utilizing any available calibrations retrieved from the file\n\n# dimension vectors -\nprint('The first dimension:')\nprint(f'  - dimension name: {datacube.dim_names[0]}')\nprint(f'  - dimension units: {datacube.dim_units[0]}')\nprint(f'  - dim vector: {datacube.dims[0][:10]}') # note the `[:10]` - we're only displaying the first 10 entries\nprint()\nprint('The third dimension:')\nprint(f'  - dimension name: {datacube.dim_names[2]}')\nprint(f'  - dimension units: {datacube.dim_units[2]}')\nprint(f'  - dim vector: {datacube.dims[2][:10]}')\n\nprint()\n\n# pixel sizes -\nqpix = datacube.calibration.get_Q_pixel_size()\nqpixunit = datacube.calibration.get_Q_pixel_units()\nrpix = datacube.calibration.get_R_pixel_size()\nrpixunit = datacube.calibration.get_R_pixel_units()\nprint()\nprint(f\"The diffraction space pixels are each {qpix:.4f} {qpixunit}\")\nprint(f\"The real space pixels are each {rpix:.4f} {rpixunit}\")\n\n# complete list of calibrations is located here; the above vectors are derived from these values\n\ndatacube.calibration\n\n# Currently, the real space pixel size is shown as 1 pixel, indicating that this information was either unavailable or not extracted from the .dm4 file.\n# Assuming we know the real space pixel size between beam positions is 5 nanometers, we can update the value using:\n\ndatacube.calibration.set_R_pixel_size(5)\ndatacube.calibration.set_R_pixel_units('nm')\n\n# and print the newly calibrated values with:\ndatacube.calibration\n\n# the values will be automatically updated in the datacube\n\n","type":"content","url":"/loadandpre#calibrate-pixel-size-and-unit","position":9},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Save the modified/unmodified data"},"type":"lvl2","url":"/loadandpre#save-the-modified-unmodified-data","position":10},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Save the modified/unmodified data"},"content":"Now we set a new filepath consisting of the old path with additions indicating our processing\n\nfrom the name alone should be clear if we preprosessed/filtered/binned\n\nadditionalls I implemented timestr to include the date\n\n# this data is saved as a .h5 file format \n\nfrom os.path import splitext\n# filepath_save = splitext(filepath_data)[0] + '_preprocessed_unfiltered_no_bin_' + timestr + '.h5'\nfilepath_save = splitext(filepath_data)[0] + '_preprocessed_unfiltered_no_bin.h5'\n\n# print the new filepath \n\nprint(filepath_save)\n\n# Save\n\npy4DSTEM.save(\n    filepath_save,\n    datacube,\n    mode = 'o'    # 'overwrite' mode\n)\n\n","type":"content","url":"/loadandpre#save-the-modified-unmodified-data","position":11},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Inspect the resulting HDF5 file"},"type":"lvl2","url":"/loadandpre#inspect-the-resulting-hdf5-file","position":12},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Inspect the resulting HDF5 file"},"content":"first we want to see where the data lives without opening it\n\nsecondly we check which default name was assigned\n\n# 'dm_dataset' and 'dm_dataset_root' are placeholders we could re-assigning a different name later\n\npy4DSTEM.print_h5_tree(filepath_save)\n\ndatacube.name","type":"content","url":"/loadandpre#inspect-the-resulting-hdf5-file","position":13},{"hierarchy":{"lvl1":"Load and examine a dataset"},"type":"lvl1","url":"/basicvis","position":0},{"hierarchy":{"lvl1":"Load and examine a dataset"},"content":"sample by HW, mesurment by CO\n\nhigh-pressure torsion sample, Ni65Cu35\n\n","type":"content","url":"/basicvis","position":1},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Set environment: Load the packages"},"type":"lvl2","url":"/basicvis#set-environment-load-the-packages","position":2},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Set environment: Load the packages"},"content":"for details on how to connect to the cluster and set up environments visit \n\npaullobpreis.com (pw:4DSTEM)\n\nimport scipy as sp\nimport numpy as np\nimport matplotlib\nimport matplotlib as plt\n\n# timestr used to create simple timestamps for easier version controll\nimport time\ntimestr = time.strftime(\"%Y%m%d\")\n\n# py4dstem as main tool, the follwing command also prints the currently used version\nimport py4DSTEM\npy4DSTEM.__version__\n\n# make dir for output images\nimport os\noutput_dir = 'output_images_' + timestr\n\nos.makedirs(output_dir)\n\n\n# Load the .dm4 file from a workspace on OMNI or XNAS, set filepath with 'filepath_data = \"FILEPATH\"'\n# even better, use dirpath'' and 'filepath_data', this makes it easier to add more paths later\n\ndirpath = \"/Users/paullobpreis/GitHub/Paullo9.github.io/data/\"\n\nfilepath_data = dirpath + 'COPL_Ni65Cu35_C_ROI3_240827_aper_50_conv_1.5_spot_6_CL_47_stepsize_10_r_x_178_r_y_186_GIF_512x512_preprocessed_unfiltered_bin_4_20240828.h5'\n\n# Examine the file contents\n\npy4DSTEM.print_h5_tree( filepath_data )\n\n# Load data into RAM\n\ndatacube = py4DSTEM.read(\n    filepath = filepath_data,\n    datapath = 'dm_dataset_root/dm_dataset'\n)\n\ndatacube\n\n","type":"content","url":"/basicvis#set-environment-load-the-packages","position":3},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Basic visualization"},"type":"lvl2","url":"/basicvis#basic-visualization","position":4},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Basic visualization"},"content":"\n\n# to make the syntax easier we import the funktion 'show' this allows us to write 'show(dp)' insted of 'py4DSTEM.show(dp)'\nfrom py4DSTEM import show\n\n# Let‚Äôs examine a single diffraction pattern.\n# We can achieve this by 'slicing' into the datacube, using two integer indices that represent the beam position (rx, ry) of interest\n\ndp = datacube.data[50,60]\n\nfig, ax = show(\n    dp,\n    figsize=(6,6),\n    returnfig = True)\n\n\nfig.savefig(os.path.join(output_dir, 'dp.pdf'))\n\n","type":"content","url":"/basicvis#basic-visualization","position":5},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Image scaling and contrast"},"type":"lvl2","url":"/basicvis#image-scaling-and-contrast","position":6},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Image scaling and contrast"},"content":"py4DSTEM attempts to automatically adjust image contrast to highlight numerous image features.\n\nmanual adjustments to scaling and contrast are often needed to reveal more image details. This can be achieved by applying a nonlinear intensity-to-color mapping, adjusting the color axis range, or both\n\nmake these modifications by adding extra arguments to the show() function\n\nTo adjust the image contrast in py4DSTEM, you can specify the color axis range, apply logarithmic scaling, or use power law scaling. Here‚Äôs how you can do it:\n\nManual Range: Set a specific intensity range.\n\nLogarithmic Scaling: Ideal for varying intensity by orders of magnitude.\n\nPower Law Scaling: Useful for fine-tuning visualization, such as excluding extremely weak features.\n\nYou can also choose different colormaps to visualize pixel intensities effectively.\n\n# intensity_range: Defines how to scale intensity ('ordered', 'absolute', 'log').\n# vmin and vmax: Specify the intensity range.\n# cmap: Choose the colormap for pixel intensity visualization.\n# Matplotlib offers a wide range of colormaps\n\nfig, ax = show(\n    dp,\n    intensity_range='ordered', # try 'absolute' or 'log'\n    vmin=0,\n    vmax=1,\n    cmap='viridis', # try 'turbo' or ',\n    figsize=(6,6),\n    aspect='auto',\n    returnfig = True)\n\n\nfig.savefig(os.path.join(output_dir, 'dp_ordered_cmap_viridis.pdf'))\n\n","type":"content","url":"/basicvis#image-scaling-and-contrast","position":7},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Mean and Maximum Diffraction Patterns"},"type":"lvl2","url":"/basicvis#mean-and-maximum-diffraction-patterns","position":8},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Mean and Maximum Diffraction Patterns"},"content":"To quickly evaluate the dataset‚Äîwhether it‚Äôs single crystal, polycrystalline, amorphous, or a mixture‚Äîwe can calculate the mean diffraction pattern.\n\nThis approach provides an overview of the entire dataset by averaging all the diffraction patterns, helping to identify the general characteristics of the sample efficiently.\n\n# Mean diffraction pattern\n\n# compute\ndp_mean = datacube.get_dp_mean()\n\n# show\nfig, ax = show(\n    dp_mean,\n    scaling='log',\n    figsize=(6,6),\n    aspect='auto',\n    returnfig = True\n)\n\nfig.savefig(os.path.join(output_dir, 'dp_mean.pdf'))\n\n# The mean diffraction pattern is stored as a native py4DSTEM datatype, \n# called 'VirtualDiffraction' class, which is basically a numpy array\n\ndp_mean\ndp_mean.data\n\n# the output will be automatically stored in a filetree-like structure for data storage\n\ndatacube.tree()\n\n# thereby the mean diffraction pattern can be always accesed by:\n\ndatacube.tree( 'dp_mean' )\n\n# and shown via:\n\nshow(\n    datacube.tree( 'dp_mean' ),\n    scaling='log',\n    figsize=(6,6),\n    aspect='auto',\n    returnfig = True\n)\n\n\nWhile calculating the mean diffraction pattern provides an overview of prominent features, such as intensity rings and Bragg disks, it can hide features that appear only in a few scan positions.\n\nTo address this, we also visualize the maximum diffraction pattern, which captures the maximum signal of each pixel across all probe positions. This approach reveals the brightest scattering from each pixel, highlighting all Bragg scattering even if it occurs in just one diffraction image\n\n# Maximum diffraction pattern\n\n# compute\ndp_max = datacube.get_dp_max()\n\n# show\nfig, ax = show(\n    dp_max,\n    vmin=0,\n    vmax=1e7,\n    figsize=(6,6),\n    aspect='auto',\n    returnfig = True  \n)\n\nfig.savefig(os.path.join(output_dir, 'dp_max.pdf'))\n\n","type":"content","url":"/basicvis#mean-and-maximum-diffraction-patterns","position":9},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Virtual imaging"},"type":"lvl2","url":"/basicvis#virtual-imaging","position":10},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Virtual imaging"},"content":"now we will generate virtual bright field (BF) and virtual dark field (DF) images\n\n","type":"content","url":"/basicvis#virtual-imaging","position":11},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl3":"Virtual Bright field (BF)","lvl2":"Virtual imaging"},"type":"lvl3","url":"/basicvis#virtual-bright-field-bf","position":12},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl3":"Virtual Bright field (BF)","lvl2":"Virtual imaging"},"content":"\n\n### Set detector geometry by hand\n\n# First we need to position the detector\n# we can set the geometry by hand\n\ncenter = (64,64)\n\nradius = 10\n\n\n# overlay selected detector position over mean dp\ndatacube.position_detector(\n    mode = 'circle',\n    geometry = (\n        center,\n        radius\n    )\n)\n\n# By default, the position_detector method overlays the selected detector on the mean diffraction pattern if it's available\n# You can also overlay it on other images, such as the maximum diffraction pattern or a specific scan position\n\ndatacube.position_detector(\n     data = dp_max,\n#     data = datacube[10,30],\n    mode = 'circle',\n    geometry = (\n        center,\n        radius\n    ),\n)\n\n","type":"content","url":"/basicvis#virtual-bright-field-bf","position":13},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl4":"Set detector geometry programmatically","lvl3":"Virtual Bright field (BF)","lvl2":"Virtual imaging"},"type":"lvl4","url":"/basicvis#set-detector-geometry-programmatically","position":14},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl4":"Set detector geometry programmatically","lvl3":"Virtual Bright field (BF)","lvl2":"Virtual imaging"},"content":"Instead of determining the center and radius by hand, we can do so programmatically by determining the position and radius of the center beam.\n\n# Find the center and probe radius\n\n\n# Get the probe position and size\nprobe_semiangle, probe_qx0, probe_qy0 = datacube.get_probe_size(\n    dp_mean.data,\n)\n\n# Overlay the computed probe over the maximum diffraction pattern\nfig, ax = show(\n    dp_max, \n    scaling='log',\n    vmin = 0,\n    vmax = 1e5,\n    circle = {\n      'center':(probe_qx0, probe_qy0),\n      'R': probe_semiangle,\n      'alpha':0.3,\n      'fill':True\n    },\n    aspect='auto',\n    returnfig = True \n)\n\n# Print the estimated probe radius\nprint('Estimated probe radius =', '%.2f' % probe_semiangle, 'pixels')\n\n# save fig\nfig.savefig(os.path.join(output_dir, 'BF_set_det_prog_dp_max.pdf'))\n\n# Capture/compute the virtual BF \n\ndatacube.get_virtual_image(\n    mode = 'circle',\n    geometry = (center,radius),\n    name = 'bright_field',       # the output will be stored in `datacube`'s tree with this name\n)\n\n# and show the result\nfig, ax = show( datacube.tree('bright_field'),\n    returnfig = True,\n        cmap='viridis'\n              )\n\n# and show the result\nfig, ax = show( datacube.tree('bright_field'),\n    returnfig = True\n              )\n\n# save fig\nfig.savefig(os.path.join(output_dir, 'BF.pdf'), bbox_inches='tight')\n\n","type":"content","url":"/basicvis#set-detector-geometry-programmatically","position":15},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl3":"Annular dark-field imaging (ADF)","lvl2":"Virtual imaging"},"type":"lvl3","url":"/basicvis#annular-dark-field-imaging-adf","position":16},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl3":"Annular dark-field imaging (ADF)","lvl2":"Virtual imaging"},"content":"\n\n# Position the detector\n\n\n# set the geometry\ncenter = probe_qx0, probe_qy0\nr_inner = probe_semiangle * 7\nr_outer = probe_semiangle * 9\nradii = r_inner,r_outer\n\n# overlay selected detector position over mean dp\ndatacube.position_detector(\n    mode = 'annular',\n    geometry = (\n        center,\n        radii\n    )\n)\n\n# Capture the virtual ADF\n\n# compute\ndatacube.get_virtual_image(\n    mode = 'annulus',\n    geometry = (center,radii),\n    name = 'annular_dark_field'\n)\n\n# show\nfig, ax = show(datacube.tree('annular_dark_field'),\n        returnfig = True,\n        cmap='viridis'\n              )\n\n# save fig\nfig.savefig(os.path.join(output_dir, 'ADF.pdf'), bbox_inches='tight')\n\n","type":"content","url":"/basicvis#annular-dark-field-imaging-adf","position":17},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Off axis dark-field imaging"},"type":"lvl2","url":"/basicvis#off-axis-dark-field-imaging","position":18},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Off axis dark-field imaging"},"content":"In traditional TEM dark-field imaging, a parallel beam illuminates the sample, and an aperture in the diffraction plane isolates electrons scattered through a specific area, producing an image from those electrons.\n\nSimilarly, we can create a virtual off-axis dark-field image by positioning a circular detector in an off-axis location in diffraction space.\n\n# Position detector\n\nqx0_DF,qy0_DF = 195,300\n\n\nr_DF = 15\n\ngeometry = (\n    (qx0_DF,qy0_DF),\n    r_DF\n)\n\ndatacube.position_detector(\n    mode = 'circular',\n    geometry = geometry\n)\n\n# Capture and display the off-axis DF image\n\ndatacube.get_virtual_image(\n    mode = 'circle',\n    geometry = geometry,\n    name = 'virt_dark_field_01'\n)\n\nfig, ax = show(datacube.tree('virt_dark_field_01'),\n              returnfig = True)\n\n# save fig \nfig.savefig(os.path.join(output_dir, 'off_axis_DF.pdf'), bbox_inches='tight')\n\n","type":"content","url":"/basicvis#off-axis-dark-field-imaging","position":19},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Virtual Diffraction"},"type":"lvl2","url":"/basicvis#virtual-diffraction","position":20},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Virtual Diffraction"},"content":"We can also create an average diffraction pattern from a subset of scan positions, revealing the scattering characteristics in specific areas of real space.\n\nPreviously, we generated virtual diffraction patterns, such as the mean and maximum, using all data points. Now, we‚Äôll compute similar patterns, but only from selected scan positions.\n\n# Select a region of real space with a circular mask.\n\n\n# set geometry\nmask_x0, mask_y0 = 145,60\nmask_radius = 10\n\n\n# plot the mask geometry, overlaid onto the dark field image we created earlier\nshow(\n    datacube.tree('annular_dark_field'),\n    circle = {\n      'center':(mask_x0, mask_y0),\n      'R': mask_radius,\n      'alpha':0.3,\n      'fill':True\n    }\n)\n\n# The virtual diffraction method expects a real-space shaped boolean mask - that is, an array with\n# values of True and False - to tell it which scan positions to use to make the virtual image.\n\n# The code below makes a mask\n\nimport numpy as np\nryy,rxx = np.meshgrid(\n    np.arange(datacube.R_Ny),\n    np.arange(datacube.R_Nx),\n)\nrrr = np.hypot( rxx-mask_x0, ryy-mask_y0 )\nmask = rrr < mask_radius\n\n# show\nshow(\n    datacube.tree('annular_dark_field'),\n    mask = mask\n)\n\n","type":"content","url":"/basicvis#virtual-diffraction","position":21},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"selected area diffraction 2"},"type":"lvl2","url":"/basicvis#selected-area-diffraction-2","position":22},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"selected area diffraction 2"},"content":"\n\n# Find the mean of the diffraction pattern in the specified region\n\nselected_area_diffraction_01 = datacube.get_virtual_diffraction(\n    method = 'mean',\n    mask = mask,\n    name = 'selected_area_diffraction_01'\n)\n\n# show\npy4DSTEM.visualize.show(\n    selected_area_diffraction_01,\n    cmap = 'viridis'\n)\n\n\n\n","type":"content","url":"/basicvis#selected-area-diffraction-2","position":23},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Write and read"},"type":"lvl2","url":"/basicvis#write-and-read","position":24},{"hierarchy":{"lvl1":"Load and examine a dataset","lvl2":"Write and read"},"content":"\n\n# set a filepath\n\ndirpath\n\nfilepath_save = dirpath + 'analysis_basics_01.h5'\nfilepath_save\n\n# inspect what's in `datacube`'s  tree - this is what we'll save!\n\ndatacube.tree()\n\n# save\n\npy4DSTEM.save(\n    filepath_save,\n    datacube,\n    tree = None,  # this indicates saving everything *under* datacube, but not not datacube itself\n    mode = 'o'    # this says that if a file of this name already exists, we'll overwrite it\n)\n\n# inspect the resulting HDF5 file\n\npy4DSTEM.print_h5_tree(filepath_save)\n\n# check that it worked as expected - load everything:\n\nd = py4DSTEM.read(filepath_save)\n\nd.tree()\n\nshow(d.tree('bright_field'))","type":"content","url":"/basicvis#write-and-read","position":25},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: Bragg disk detection"},"type":"lvl1","url":"/diskdetec","position":0},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: Bragg disk detection"},"content":"","type":"content","url":"/diskdetec","position":1},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: Bragg disk detection","lvl4":"Acknowledgements"},"type":"lvl4","url":"/diskdetec#acknowledgements","position":2},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: Bragg disk detection","lvl4":"Acknowledgements"},"content":"This tutorial was created by the py4DSTEM instructor team:\n\nBen Savitzky (\n\nbhsavitzky@lbl.gov)\n\nSteve Zeltmann (\n\nsteven‚Äã.zeltmann@berkeley‚Äã.edu)\n\nStephanie Ribet (\n\nsribet@u‚Äã.northwestern‚Äã.edu)\n\nAlex Rakowski (\n\narakowski@lbl.gov)\n\nColin Ophus (\n\nclophus@lbl.gov)\n\nUpdated 11/1/2023, version 0.14.8\n\n","type":"content","url":"/diskdetec#acknowledgements","position":3},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: Bragg disk detection","lvl3":"Set up the environment"},"type":"lvl3","url":"/diskdetec#set-up-the-environment","position":4},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: Bragg disk detection","lvl3":"Set up the environment"},"content":"\n\nimport scipy as sp\nimport numpy as np\nimport matplotlib\nimport matplotlib as plt\n\n\n# timestr used to create simple timestamps for easier version controll\nimport time\ntimestr = time.strftime(\"%Y%m%d\")\n\n\n# output dir for images,\nimport os\noutput_dir = 'output_images_' + timestr\n\n\nimport py4DSTEM\n\nfrom py4DSTEM.visualize import show\npy4DSTEM.__version__\n\n","type":"content","url":"/diskdetec#set-up-the-environment","position":5},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: Bragg disk detection","lvl2":"Download the tutorial data "},"type":"lvl2","url":"/diskdetec#download-the-tutorial-data","position":6},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: Bragg disk detection","lvl2":"Download the tutorial data "},"content":"You can download the tutorial dataset here: (501 megabytes)\n\nSimulated single crystal and polycrystalline gold\n\n","type":"content","url":"/diskdetec#download-the-tutorial-data","position":7},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: Bragg disk detection","lvl2":"Load data"},"type":"lvl2","url":"/diskdetec#load-data","position":8},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: Bragg disk detection","lvl2":"Load data"},"content":"Load\n\nthe same datacube as in the previous notebook, and\n\nall the results produced in that notebook\n\n# Load the .dm4 file from a workspace on OMNI or XNAS, set filepath with 'filepath_data = \"FILEPATH\"'\n# even better, use dirpath'' and 'filepath_data', this makes it easier to add more paths later\n\ndirpath = \"/fast/ws-tmp/g031362-data/high_pressure_torsion/lamC/ROI3/\" #OMNI\n\nfilepath_data = dirpath + 'COPL_Ni65Cu35_C_ROI3_240827_aper_50_conv_1.5_spot_6_CL_47_stepsize_10_r_x_178_r_y_186_GIF_512x512_preprocessed_unfiltered_no_bin.h5'\nfilepath_basics_01 = dirpath + 'analysis_basics_01.h5'\n\n\n# Load the datacube\n\ndatacube = py4DSTEM.read(\n    filepath = filepath_data,\n    datapath = 'dm_dataset_root/dm_dataset'\n)\n\ndatacube\n\n# Load the prior analysis\n\nbasics_01 = py4DSTEM.read(\n    filepath_basics_01\n)\n\nbasics_01.tree()\n\n","type":"content","url":"/diskdetec#load-data","position":9},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: Bragg disk detection","lvl2":"Basic visualization"},"type":"lvl2","url":"/diskdetec#basic-visualization","position":10},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: Bragg disk detection","lvl2":"Basic visualization"},"content":"Show some of the data retrieved from the previous analysis\n\n# Mean diffraction\n\nshow(\n    basics_01.tree('dp_mean'),\n    scaling='log',\n)\n\n# Max diffraction\n\nshow(\n    basics_01.tree('dp_max'),\n    scaling='log',\n)\n\n# Virtual BF\n\n# show\nshow( basics_01.tree( 'bright_field' ) )\n\n","type":"content","url":"/diskdetec#basic-visualization","position":11},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: Bragg disk detection","lvl2":"Disk detection"},"type":"lvl2","url":"/diskdetec#disk-detection","position":12},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: Bragg disk detection","lvl2":"Disk detection"},"content":"We‚Äôll perform disk detection using template matching.  The mathematical machinery here is the cross correlation - if we compute the cross correlation of a template with an image, the maxima of the resulting correlogram correspond to the positions in the image that are good matches to the template.\n\nThe steps are:\n\nGenerate a template (the vacuum electron probe)\n\nPrepare the template for cross correlation (generate a cross correlation kernel from the probe image)\n\nSelect a few scan position and test/refine parameters for the disk detection algorithm\n\nFind all the disks\n\n","type":"content","url":"/diskdetec#disk-detection","position":13},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: Bragg disk detection","lvl3":"Generage a probe template","lvl2":"Disk detection"},"type":"lvl3","url":"/diskdetec#generage-a-probe-template","position":14},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: Bragg disk detection","lvl3":"Generage a probe template","lvl2":"Disk detection"},"content":"There are a number of ways to get a probe.  If you‚Äôre running an experiment, the best thing to do is to collect a separate dataset of the probe over vacuum.  It need not be large, but collecting at least a few or up to ~a hundred patterns to average over is useful.\n\nHere, we‚Äôll select a region of the scan we have that‚Äôs approximately over vacuum - it contains no gold particles, so only the thin amorphous support should be present - and use these scan positions to compute our probe.\n\n# select a vacuum region\n\nimport numpy as np\nmask = np.zeros(datacube.Rshape,dtype=bool)\nmask[175:180,105:110] = 1\n\n\nshow(\n    basics_01.tree('bright_field'),\n    mask = ~mask,\n    mask_alpha = 0.667,\n    mask_color = 'r'\n)\n\n# generate a probe\n\nprobe = datacube.get_vacuum_probe( ROI=mask )\n\nshow(\n    probe.probe,\n    scaling='none',\n    intensity_range='absolute'\n)\n\n# Find the center and semiangle\n\nalpha_pr,qx0_pr,qy0_pr = datacube.get_probe_size( probe.probe )\n\nshow(\n    probe.probe, \n    scaling='none',\n    intensity_range='absolute',\n    vmin=0,vmax=7.2e5,\n    circle = {\n      'center':(qx0_pr,qy0_pr),\n      'R': alpha_pr,\n      'alpha':0.3,\n      'fill':True\n    }\n)\n\nThe probe kernel is what we cross correlate each diffraction pattern with.  The simplest kernel would involve just shifting the vacuum probe to the origin.  However, disk detection is typically improved if we use a slightly more sophisticated kernel - one in which we carve out a narrow trench of negative intensity encircling the probe. This acts like an edge filter when we do template matching, helping ‚Äúlock in‚Äù the template on the center each disk by penalizing template positions which are slightly offset.\n\n# prepare the probe kernel\n\nprobe.get_kernel(\n    mode='sigmoid',\n    origin=(qx0_pr,qy0_pr),\n    radii=(alpha_pr,2*alpha_pr)   # the inner and outer radii of the 'trench'\n)\n\nfig, ax = py4DSTEM.visualize.show_kernel(\n    probe.kernel,\n    R = 24,\n    L = 24,\n    W = 1,\n    returnfig = True\n)\n\nfig.savefig(os.path.join(output_dir, 'probe_kernel.pdf'), bbox_inches='tight')\n\n# Select a few sample patterns for parameter tuning\n\n\n# choose scan positions\n\nrxs = 20,60,80,110,110,100\nrys = 5,35,41,39,90,25\n\n\n\n# visualize\n\ncolors=['r','limegreen','c','g','orange', 'violet']\n\n# show the selected\n# positions in real space\nfig, ax = py4DSTEM.visualize.show_points(\n    basics_01.tree('bright_field'),\n    x=rxs,\n    y=rys,\n    scale=300,\n    pointcolor=colors,\n    figsize=(8,8),\n    returnfig = True\n)\n\n# save fig\nfig.savefig(os.path.join(output_dir, 'show_points.pdf'), bbox_inches='tight')\n\n# show the selected\n# diffraction patterns\nfig, ax = py4DSTEM.visualize.show_image_grid(\n    get_ar = lambda i:datacube[rxs[i],rys[i],:,:],\n    H=2,W=3,\n    axsize=(5,5),\n    intensity_range='absolute',\n    vmin=10,\n    vmax=1e5,\n    scaling='power',\n    power=0.75,\n    get_bordercolor = lambda i:colors[i],\n    returnfig = True\n)\n\n# save fig\nfig.savefig(os.path.join(output_dir, 'image_grid.pdf'), bbox_inches='tight')\n\n# Choose disk detection parameters\n\n\n# set parameters\n\ndetect_params = {\n    'minAbsoluteIntensity': 8,   # intensity threshold\n    'minRelativeIntensity': 0,   # int. thresh. relative to brightest disk in each pattern\n    'minPeakSpacing': 15,         # if two peaks are closer than this (in pixels), remove the dimmer peak\n    'edgeBoundary': 2,           # remove peaks within this distance of the edge of the diffraction pattern\n    'sigma': 0.4,                  # gaussian blur size to apply to cross correlation before finding maxima\n    'maxNumPeaks': 30,          # maximum number of peaks to return, in order of intensity\n    'subpixel' : 'poly',         # subpixel resolution method\n    'corrPower': 1.0,            # if <1.0, performs a hybrid cross/phase correlation. More sensitive to edges and to noise\n#     'CUDA': True,              # if a GPU is configured and cuda dependencies are installed, speeds up calculation \n}\n\n\n# find disks for selected patterns\ndisks_selected = datacube.find_Bragg_disks(\n    data = (rxs, rys),\n    template = probe.kernel,\n    **detect_params,\n)\n\n# show\nfig, ax = py4DSTEM.visualize.show_image_grid(\n    get_ar = lambda i:datacube[rxs[i],rys[i],:,:],\n    H=2, \n    W=3,\n    axsize=(5,5),\n    intensity_range='absolute',\n    scaling='power',\n    power=0.5,\n    get_bordercolor = lambda i:colors[i],\n    get_x = lambda i: disks_selected[i].data['qx'],\n    get_y = lambda i: disks_selected[i].data['qy'],\n    get_pointcolors = lambda i: colors[i],\n    open_circles = True,\n    scale = 650,\n    returnfig = True\n)\n\n# save fig\nfig.savefig(os.path.join(output_dir, 'disc_detection.pdf'), bbox_inches='tight')\n\n# compute for all diffraction patterns\n\nbraggpeaks = datacube.find_Bragg_disks(\n    template = probe.kernel,\n    **detect_params,\n)\n\nLocalizing the disks measures the Bragg points.  Just like the max and mean diffraction patterns allow us to get a succinct but highly informative picture of the whole dataset all at once, it‚Äôs useful to condense all the information about Bragg scattering we just measured into a single visualization.  We do this with a Bragg vector map - a 2D histogram of all the Bragg vector positions and intensities we‚Äôve measured.\n\n# Bragg vector map\n\n# compute\nbvm = braggpeaks.histogram( mode='raw' )\n\n# show\nshow(bvm)\n\n# Increasing the sampling can be helpful to visualize the resolution and error of our measurement.\n\n# compute\nbvm_upsampled = braggpeaks.histogram(\n    mode='raw',\n    sampling = 8\n)\n\n# show\nshow(bvm_upsampled)\n\n","type":"content","url":"/diskdetec#generage-a-probe-template","position":15},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: Bragg disk detection","lvl2":"Write and read"},"type":"lvl2","url":"/diskdetec#write-and-read","position":16},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: Bragg disk detection","lvl2":"Write and read"},"content":"\n\n# set a filepath\n\nfilepath_save = dirpath + 'analysis_basics_02.h5'\nfilepath_save\n\n# inspect what's in `datacube`'s  tree\n\ndatacube.tree()\n\n# save everthing except the datacube\n\npy4DSTEM.save(\n    filepath_save,\n    datacube,\n    tree = None,  # everything *under* datacube, but not not datacube itself\n    mode = 'o'\n)\n\n# inspect the resulting HDF5 file\n\npy4DSTEM.print_h5_tree(filepath_save)\n\n# Load to confirm everything worked\n\nbasics_02 = py4DSTEM.read(\n    filepath_save\n)\n\nbasics_02.tree()","type":"content","url":"/diskdetec#write-and-read","position":17},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration"},"type":"lvl1","url":"/cali","position":0},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration"},"content":"There are many calibrations that can be performed on 4DSTEM data.  Which ones are necessary depends on the data and the analysis being performed.  The most typical calibrations are\n\nthe origin\n\nthe ellipticity\n\nthe pixel size\n\nIn this notebook we‚Äôll perform these three calibration.\n\nThere are various methods for performing calibrations.  Here we‚Äôll calibrate using the Bragg vectors detected in the previous notebook.","type":"content","url":"/cali","position":1},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration","lvl4":"Acknowledgements"},"type":"lvl4","url":"/cali#acknowledgements","position":2},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration","lvl4":"Acknowledgements"},"content":"This tutorial was created by the py4DSTEM instructor team:\n\nBen Savitzky (\n\nbhsavitzky@lbl.gov)\n\nSteve Zeltmann (\n\nsteven‚Äã.zeltmann@berkeley‚Äã.edu)\n\nStephanie Ribet (\n\nsribet@u‚Äã.northwestern‚Äã.edu)\n\nAlex Rakowski (\n\narakowski@lbl.gov)\n\nColin Ophus (\n\nclophus@lbl.gov)\n\nUpdated 11/1/2023, version 0.14.8\n\n","type":"content","url":"/cali#acknowledgements","position":3},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration","lvl3":"Set up the environment"},"type":"lvl3","url":"/cali#set-up-the-environment","position":4},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration","lvl3":"Set up the environment"},"content":"\n\nimport py4DSTEM\nimport time\nimport os\nfrom py4DSTEM.visualize import show\n\ntimestr = time.strftime(\"%Y%m%d\")\n#timestr = '20240813' # in case you want to use older data/not preprocess everything again\n\npy4DSTEM.__version__\n\n","type":"content","url":"/cali#set-up-the-environment","position":5},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration","lvl2":"Load data"},"type":"lvl2","url":"/cali#load-data","position":6},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration","lvl2":"Load data"},"content":"Load\n\nthe same datacube as in the previous notebook, and\n\nthe results from basics_02 (disk detection)\n\n# Load the .dm4 file from a workspace on OMNI or XNAS, set filepath with 'filepath_data = \"FILEPATH\"'\n# even better, use dirpath'' and 'filepath_data', this makes it easier to add more paths later\n\ndirpath = \"/fast/ws-tmp/g031362-data/high_pressure_torsion/lamC/ROI3/\"\n\nfilepath_data = dirpath + 'COPL_Ni65Cu35_C_ROI3_240827_aper_50_conv_1.5_spot_6_CL_47_stepsize_10_r_x_178_r_y_186_GIF_512x512_preprocessed_unfiltered_no_bin.h5'\nfilepath_basics_01 = dirpath + 'analysis_basics_01.h5'\nfilepath_basics_02 = dirpath + 'analysis_basics_02.h5'\n\n# Load the datacube\n\ndatacube = py4DSTEM.read(\n    filepath = filepath_data,\n    datapath = 'dm_dataset_root/dm_dataset'\n)\n\ndatacube\n\n# Load the prior analysis\n\nbasics_02 = py4DSTEM.read(\n    filepath_basics_02\n)\n\nbasics_02.tree()\n\n","type":"content","url":"/cali#load-data","position":7},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration","lvl2":"Examine the Bragg vectors"},"type":"lvl2","url":"/cali#examine-the-bragg-vectors","position":8},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration","lvl2":"Examine the Bragg vectors"},"content":"\n\n# Let's assign the bragg vectors a variable for convenience\n\nbraggpeaks = basics_02.tree('braggvectors')\nbraggpeaks\n\n# Data can be retrieved from BraggVectors instances in two places: .raw, and .cal\n# Let's look at .raw first:\n\nbraggpeaks.raw\n\nbraggpeaks.raw[0,0]\n\nprint(braggpeaks.raw[0,0].qx)\nprint()\nprint(braggpeaks.raw[0,0].qy)\nprint()\nprint(braggpeaks.raw[0,0].I)\n\n# The .data property gives us everything (positions and intensities) in a single numpy array.\n# It's a structured array, meaning it has a 'dtype' containing several fields,\n# each with their own datatype\n\nbraggpeaks.raw[0,0].data\n\nbraggpeaks.raw[0,0].data['qx']\n\n# The calibrated vectors at .cal have the same interface\n# The only difference is that these vectors are transformed before\n# being returned, where the transformation is determined\n# by the BraggVector's calibration state\n\nbraggpeaks.cal\n\n# We haven't made any calibration measurements yet, and none\n# were retrieved from microscopy metadata (like pixel sizes),\n# so currently no calibrations are applied and the .cal and\n# .raw vectors are identical.\n\nbraggpeaks.cal[0,0].data\n\n# You can check the current calibration state\n\nbraggpeaks.calstate\n\n# The calibration state should update automatically as new calibrations measurements\n# are taken.  You can also override these and set which calibrations you want applied\n# when .cal is used with the .setcal method\n\nbraggpeaks.setcal?\n\n# Calling .setcal method with no arguments are supplied sets the calibration state\n# based on what's available in the dataset's Calibration metadata (i.e. at .calibation)\n\nbraggpeaks.setcal()\n\n# All the calibrations are still set to False because we haven't performed\n# any calibration measurements yet!\n\nbraggpeaks.calstate\n\nbraggpeaks.calibration\n\n# If we try to set the braggvector's calibration state to perform a transformation\n# for which is can't find the needed measurement in the Calibration metadata, it\n# raises an error\n\n# braggpeaks.setcal(\n#    center = True,       # This can't be True without first setting the origin position!\n#    ellipse = False,\n#    pixel = False,\n#    rotate = False\n#)\n\n# Let's confirm that the raw vectors represent the positions of\n# detected bragg disks in a sample diffraction pattern\n\ndp = datacube[60,75]\nv = braggpeaks.raw[60,75]\n\nshow(\n    dp,\n    points = {\n        'x' : v.qx,\n        'y' : v.qy,\n    }\n)\n\n# compute the bragg vector map\n\nbvm = braggpeaks.histogram(\n    mode='raw',\n    sampling = 8,\n)\n\nshow(bvm)\n\n# The bragg vector map is another py4DSTEM class which contains various self-descriptive metadata items\n\nbvm\n\nprint(bvm.origin)\nprint(bvm.pixelsize)\nprint(bvm.pixelunits)\n\nbvm.dims[0]\n\n","type":"content","url":"/cali#examine-the-bragg-vectors","position":9},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration","lvl2":"Calibrate the origin"},"type":"lvl2","url":"/cali#calibrate-the-origin","position":10},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration","lvl2":"Calibrate the origin"},"content":"We‚Äôll calibrate the origin in two steps:\n\nmeasure the origin position using the brightest disk from the disk detection\n\nfit a plane to those measurements\n\n# Measure the origin\n\nqx0_meas,qy0_meas,mask_meas = braggpeaks.measure_origin()\n\nshow(\n    [qx0_meas,qy0_meas],\n    cmap = 'viridis',\n    mask = mask_meas\n)\n\n# Note that braggpeaks.measure_origin added its results to the Calibration metadata\n\nbraggpeaks.calibration\n\n# Here's how to retrieve the data:\n\nbraggpeaks.calibration.get_origin_meas()\n\n# These arrays are identical to the ones we just found\n\nprint( braggpeaks.calibration.get_origin_meas()[0] is qx0_meas )\nprint( braggpeaks.calibration.get_origin_meas()[1] is qy0_meas )\n\n# Fit a plane to the origins\n\nqx0_fit,qy0_fit,qx0_residuals,qy0_residuals = braggpeaks.fit_origin()\n\n# The fit origins now live in the calibration metadata as 'qx0','qy0'\n\nbraggpeaks.calibration\n\n# The calibration state of our BraggVectors has automatically updated\n\nbraggpeaks.calstate\n\n# and calling .cal now gives us the centered vector positions\n\nbraggpeaks.cal[0,0].data\n\nbraggpeaks.raw[0,0].data\n\n# Now that we've calibrated the center positions, we can re-compute\n# the Bragg vector map, this time with the center correction applied\n\nsampling = 8\n\n# compute\nbvm = braggpeaks.histogram(\n    #mode='cal',             # 'cal' is the default mode, so this line can be included or left out\n    sampling = sampling,\n)\n\n# show\n# overlay a circle around the center for visualization purposes\nshow(\n    bvm,\n    circle={\n        'center' : bvm.origin,   # the centered BVM knows where its origin is \n        'R' : 4*sampling,\n        'fill' : False,\n        'linewidth' : 1\n    },\n    #vmax=0.9\n)\n\n# Compare this to the uncalibrated BVM - much better!\n\n# compute raw vs. centered\nbvm_r = braggpeaks.histogram( mode='raw', sampling=sampling )\nbvm_c = braggpeaks.histogram( mode='cal', sampling=sampling )\n\n# show\nshow( [bvm_r, bvm_c] ,vmax=1e5)\n\n# show, zooming in on origin\nL = 50\nx,y = bvm_c.origin\nimport numpy as np\nx0,xf = np.round([x-L,x+L]).astype(int)\ny0,yf = np.round([y-L,y+L]).astype(int)\n\nshow(\n    [\n    bvm_r[x0:xf,y0:yf],\n    bvm_c[x0:xf,y0:yf]\n    ],\n    vmax=1e5\n)\n\n# This output is fun.  What's going on here?\n\nprint(bvm_c.pixelsize)\nprint(bvm_c.pixelunits)\n\n","type":"content","url":"/cali#calibrate-the-origin","position":11},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration","lvl2":"Calibrate the ellipticity"},"type":"lvl2","url":"/cali#calibrate-the-ellipticity","position":12},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration","lvl2":"Calibrate the ellipticity"},"content":"\n\n# Select an annular region in which to perform a fit\n# The ideal is a single, isolated ring of peaks\n\n#q_range = (310, 384)\nq_range = (520, 650)\n\npy4DSTEM.show(\n    bvm_c,\n    cmap='gray',\n    intensity_range='absolute',\n    vmin=0,\n    vmax=1e2,\n    annulus={\n        'center':bvm_c.origin,\n        'radii': q_range,'fill':True,'color':'r','alpha':0.3}\n)\n\n# Fit the elliptical distortions\np_ellipse = py4DSTEM.process.calibration.fit_ellipse_1D(\n    bvm_c,\n    center = bvm_c.origin,\n    fitradii = q_range,\n)\n\n# plot the fit\npy4DSTEM.visualize.show_elliptical_fit(\n    bvm_c,\n    q_range,\n    p_ellipse,\n    cmap='gray',\n    intensity_range='absolute',\n    vmin=800,\n    vmax=1e3,\n)\n\np_ellipse\n\n# The elliptical parameters are not automatically added to the calibration metadata,\n# (to allow inspection of the fit to ensure it's accurate), so need to be added manually\n# once a good fit is found. Like so:\n\nbraggpeaks.calibration.set_p_ellipse(p_ellipse)\n\n# Note that the code above only adds (a,b,theta) to the calibration metadata; the origin needs to\n# be calibrated separately, as we did above \n\nbraggpeaks.calibration\n\n# Calibrate, compute a new bragg vector map, and compare\n\nbraggpeaks.setcal()\nbvm_e = braggpeaks.histogram(\n    sampling=sampling\n)\n\nshow([bvm_e, bvm_r],vmax=0.99)\n\n","type":"content","url":"/cali#calibrate-the-ellipticity","position":13},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration","lvl2":"Calibrate the detector pixel size"},"type":"lvl2","url":"/cali#calibrate-the-detector-pixel-size","position":14},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration","lvl2":"Calibrate the detector pixel size"},"content":"","type":"content","url":"/cali#calibrate-the-detector-pixel-size","position":15},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration","lvl3":"Gold","lvl2":"Calibrate the detector pixel size"},"type":"lvl3","url":"/cali#gold","position":16},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration","lvl3":"Gold","lvl2":"Calibrate the detector pixel size"},"content":"The pixel size should be calibrated from a known standard.  In this case, our data is of Au nanoparticles, so we can use the scattering from the data itself for calibration.  For experiments examining samples of unknown structure, it is best practice is to record a separate scan of a standard sample, such as Au nanoparticles, for each set of microscope conditions used.  Calibration scans should be performed during the same session as the experimental scans.\n\nFor data that requires elliptical calibrations, the pixel size should be calibrated last, as prior calibrations like ellipticity will influence the pixel size.  Before calibrating and correcting the ellipticity, the true pixel size was effectively variable by position and direction in diffraction space!\n\nBelow, we calibrate the pixel size by computing the structure factors of gold, and matching them to our detected bragg scattering\n\n# Define NiCu structure \n\n# Set lattice parameters for graphite (in Angstroms)\na_lat = 3.6  # a-axis for the hexagonal lattice\natom_num = 28\n\n# Set max scattering angle, in inverse Angstroms\nk_max = 1.5\n\n# Define the atomic positions in the FCC lattice\n# FCC positions in a cubic cell:\n# (0,0,0), (0.5,0.5,0), (0.5,0,0.5), (0,0.5,0.5)\npos = np.array([\n    [0.0, 0.0, 0.0],   # Position 1\n    [0.5, 0.5, 0.0],   # Position 2\n    [0.5, 0.0, 0.5],   # Position 3\n    [0.0, 0.5, 0.5],   # Position 4\n])\n\n# Make crystal for FCC NiCu alloy\ncrystal = py4DSTEM.process.diffraction.Crystal(\n    pos,\n    atom_num,\n    a_lat\n)\n\n# Calculate structure factors for FCC NiCu alloy\ncrystal.calculate_structure_factors(k_max)\n\n# Show scattering intensity\ncrystal.plot_scattering_intensity()\n\n\n# Make an initial guess at the pixel size to refine\n# Let's estimate eith an overlay of the measured scattering and reference crystal structure.\n\n\n# Modify `pixel_size_inv_Ang_guess` until it \n# looks close before attempting to fit the data!\npixel_size_inv_Ang_guess = 0.005\n\n\n# calibrate\nbraggpeaks.calibration.set_Q_pixel_size(pixel_size_inv_Ang_guess)\nbraggpeaks.calibration.set_Q_pixel_units('A^-1')\nbraggpeaks.setcal()\n\n# show overlay\ncrystal.plot_scattering_intensity(\n    bragg_peaks = braggpeaks,\n    bragg_k_power = 2.0\n)\n\n# fit pixel size to lattice\n\ncrystal.calibrate_pixel_size(\n    bragg_peaks = braggpeaks,\n    bragg_k_power = 2.0,\n    plot_result = True,\n);\n\nbraggpeaks.calibration\n\n# New bvm, compare\n\nbraggpeaks.setcal()\nbvm_p = braggpeaks.histogram(\n    sampling=sampling\n)\n\nshow([bvm_p, bvm_r],vmax=0.99)\n\nbvm_p\n\n","type":"content","url":"/cali#gold","position":17},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration","lvl2":"Write and read"},"type":"lvl2","url":"/cali#write-and-read","position":18},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: calibration","lvl2":"Write and read"},"content":"\n\n# set a filepath\n\nfilepath_basics_03 = dirpath + 'analysis_basics_03.h5'\nfilepath_basics_03\n\n# inspect what's in the data tree\n\nbasics_02.tree()\n\n# we can see above that the bragg vector map doesn't get added automatically to the tree.\n# just for fun, let's do that now:\n\nbasics_02.tree(\n    bvm_p\n)\n\nbasics_02.tree()\n\n# save\n\npy4DSTEM.save(\n    filepath_basics_03,\n    basics_02,\n    tree=None,\n    mode = 'o'\n)\n\n# inspect the resulting HDF5 file\n\npy4DSTEM.print_h5_tree(filepath_basics_03)\n\n# read\n\nd = py4DSTEM.read(\n    filepath_basics_03,\n)\n\nd.tree()\n\nd.tree('braggvectors')\n\n# check that all the calibrations loaded correctly\n\nd.tree('braggvectors').calibration","type":"content","url":"/cali#write-and-read","position":19},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain"},"type":"lvl1","url":"/strain","position":0},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain"},"content":"In this notebook we find the strain on a gold nanoplatelet sample.\n\nThis data was simulated together with the gold nanoparticle dataset used in the basics 01-03 tutorial notebooks.  Here, we use the ellipticity and pixel size calibrations from notebook 03, then independently calibrate the origin and rotation for this dataset, before finally finding the strain.","type":"content","url":"/strain","position":1},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl4":"Acknowledgements"},"type":"lvl4","url":"/strain#acknowledgements","position":2},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl4":"Acknowledgements"},"content":"This tutorial was created by the py4DSTEM instructor team:\n\nBen Savitzky (\n\nbhsavitzky@lbl.gov)\n\nSteve Zeltmann (\n\nsteven‚Äã.zeltmann@berkeley‚Äã.edu)\n\nStephanie Ribet (\n\nsribet@u‚Äã.northwestern‚Äã.edu)\n\nAlex Rakowski (\n\narakowski@lbl.gov)\n\nColin Ophus (\n\nclophus@lbl.gov)\n\nUpdated 11/1/2023, version 0.14.8\n\n","type":"content","url":"/strain#acknowledgements","position":3},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl3":"Set up the environment"},"type":"lvl3","url":"/strain#set-up-the-environment","position":4},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl3":"Set up the environment"},"content":"\n\nimport time\nimport os\n\nimport py4DSTEM\nfrom py4DSTEM.visualize import show\nimport matplotlib.pyplot as plt\n\npy4DSTEM.__version__\n\n","type":"content","url":"/strain#set-up-the-environment","position":5},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl2":"Download the tutorial data "},"type":"lvl2","url":"/strain#download-the-tutorial-data","position":6},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl2":"Download the tutorial data "},"content":"You can download the tutorial dataset here: (501 megabytes)\n\nSimulated single crystal and polycrystalline gold\n\n","type":"content","url":"/strain#download-the-tutorial-data","position":7},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl2":"Load data"},"type":"lvl2","url":"/strain#load-data","position":8},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl2":"Load data"},"content":"Load\n\nthe same datacube as in the previous notebook, and\n\nthe results from basics_02 (disk detection)\n\n# Load the .dm4 file from a workspace on OMNI or XNAS, set filepath with 'filepath_data = \"FILEPATH\"'\n# even better, use dirpath'' and 'filepath_data', this makes it easier to add more paths later\n\ndirpath = \"/fast/ws-tmp/g031362-data/high_pressure_torsion/lamC/ROI3/\"\n\nfilepath_data = dirpath + 'COPL_Ni65Cu35_C_ROI3_240827_aper_50_conv_1.5_spot_6_CL_47_stepsize_10_r_x_178_r_y_186_GIF_512x512_preprocessed_unfiltered_no_bin.h5'\nfilepath_basics_01 = dirpath + 'analysis_basics_01.h5'\nfilepath_basics_02 = dirpath + 'analysis_basics_02.h5'\nfilepath_basics_03 = dirpath + 'analysis_basics_03.h5'\n\npy4DSTEM.print_h5_tree( filepath_data )\n\n# Load the nanoplatelet datacube\n\ndatacube = py4DSTEM.read(\n    filepath = filepath_data,\n    datapath = 'dm_dataset_root/dm_dataset'\n)\n\ndatacube\n\n# Load the defocused CBED image\n\ndefocused_cbed = py4DSTEM.read(\n    filepath = filepath_data,\n    datapath = 'dm_dataset_root/dm_dataset'\n)\n\ndefocused_cbed\n\n# Load the prior analysis\n\nbasics_03 = py4DSTEM.read(\n    filepath_basics_03\n)\n\nbasics_03.tree()\n\n","type":"content","url":"/strain#load-data","position":9},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl3":"Transfer calibrations","lvl2":"Load data"},"type":"lvl3","url":"/strain#transfer-calibrations","position":10},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl3":"Transfer calibrations","lvl2":"Load data"},"content":"\n\nCOPL_HPT = basics_03.tree('braggvectors')\n\nCOPL_HPT.calibration\n\ndatacube.calibration\n\ndatacube.calibration.set_Q_pixel_size( COPL_HPT.calibration.get_Q_pixel_size() )\ndatacube.calibration.set_Q_pixel_units( COPL_HPT.calibration.get_Q_pixel_units() )\ndatacube.calibration.set_ellipse( COPL_HPT.calibration.get_ellipse() )\n\ndatacube.calibration\n\n","type":"content","url":"/strain#transfer-calibrations","position":11},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl2":"Basic visualizations"},"type":"lvl2","url":"/strain#basic-visualizations","position":12},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl2":"Basic visualizations"},"content":"\n\n# mean + max diffraction\n\ndp_mean = datacube.get_dp_mean()\ndp_max = datacube.get_dp_max()\n\nshow(\n    [dp_mean,dp_max]\n)\n\n# virtual imaging - position detector\n\ncenter = 250,250\nradius = 30\n\ndatacube.position_detector(\n    mode = 'circle',\n    geometry = (center,radius)\n)\n\n# virtual imaging - capture images\n\nim_bf = datacube.get_virtual_image(\n    mode = 'circle',\n    geometry = (center,radius)\n)\nim_adf = datacube.get_virtual_image(\n    mode = 'annulus',\n    geometry = (center,(radius,radius*10))\n)\n\nshow(\n    [im_bf,im_adf],\n    cmap = 'viridis'\n)\n\n","type":"content","url":"/strain#basic-visualizations","position":13},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl2":"Find Bragg vectors"},"type":"lvl2","url":"/strain#find-bragg-vectors","position":14},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl2":"Find Bragg vectors"},"content":"\n\nprobe = basics_03.tree('probe')\n\nprobe\n\npy4DSTEM.visualize.show_kernel(\n    probe.kernel,\n    R = 24,\n    L = 24,\n    W = 1\n)\n\n# Select a few sample patterns for parameter tuning\n\n\n# choose scan positions\n\nrxs = 20,60,110,80,100,34\nrys = 75,35,41,39,90,68,\n\n\n\n# visualize\n\ncolors=['r','limegreen','c','g','orange', 'violet']\n\n# show the selected\n# positions in real space\npy4DSTEM.visualize.show_points(\n    im_bf,\n    x=rxs,\n    y=rys,\n    scale=400,\n    pointcolor=colors,\n    figsize=(8,8)\n)\n\n# show the selected\n# diffraction patterns\npy4DSTEM.visualize.show_image_grid(\n    get_ar = lambda i:datacube[rxs[i],rys[i],:,:],\n    H=2,W=3,\n    axsize=(5,5),\n    intensity_range='absolute',\n    vmin=1,\n    vmax=1e5,\n    scaling='power',\n    power=0.5,\n    get_bordercolor = lambda i:colors[i],\n)\n\n# Choose disk detection parameters\n\n\n# set parameters\n\ndetect_params = {\n    'minAbsoluteIntensity': 8,\n    'minRelativeIntensity': 0,\n    'minPeakSpacing': 11,\n    'edgeBoundary': 2,\n    'sigma': 0.4,\n    'maxNumPeaks': 20,\n    'subpixel' : 'poly',\n    'corrPower': 1.0,\n#     'CUDA': True,\n}\n\n\n# find disks for selected patterns\ndisks_selected = datacube.find_Bragg_disks(\n    data = (rxs, rys),\n    template = probe.kernel,\n    **detect_params,\n)\n\n# show\npy4DSTEM.visualize.show_image_grid(\n    get_ar = lambda i:datacube[rxs[i],rys[i],:,:],\n    H=2, \n    W=3,\n    axsize=(5,5),\n    intensity_range='absolute',\n    vmin=1,\n    vmax=1e5,\n    scaling='power',\n    power=0.5,\n    get_bordercolor = lambda i:colors[i],\n    get_x = lambda i: disks_selected[i].data['qx'],\n    get_y = lambda i: disks_selected[i].data['qy'],\n    get_pointcolors = lambda i: colors[i],\n    open_circles = True,\n    scale = 700,\n)\n\n# compute for all diffraction patterns\n\nbraggpeaks = datacube.find_Bragg_disks(\n    template = probe.kernel,\n    **detect_params,\n)\n\n# Bragg vector map\n\n# compute\nbvm = braggpeaks.histogram( mode='raw' )\n\n# show\nshow(bvm)\n\n\n","type":"content","url":"/strain#find-bragg-vectors","position":15},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl2":"Calibrate the origin"},"type":"lvl2","url":"/strain#calibrate-the-origin","position":16},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl2":"Calibrate the origin"},"content":"\n\n# Measure the origin\n\nqx0_meas,qy0_meas,mask_meas = braggpeaks.measure_origin()\n\nshow(\n    [qx0_meas,qy0_meas],\n    cmap = 'coolwarm',\n    mask = mask_meas\n)\n\n\n# Note that braggpeaks.measure_origin added its results to the Calibration metadata\n\nbraggpeaks.calibration\n\n# Fit a plane to the origins\n\nqx0_fit,qy0_fit,qx0_residuals,qy0_residuals = braggpeaks.fit_origin()\n\n# The fit origins now live in the calibration metadata as 'qx0','qy0'\n\nbraggpeaks.calibration\n\n# The calibration state of our BraggVectors has automatically updated\n\nbraggpeaks.calstate\n\n# Compare this to the uncalibrated BVM - much better!\n\n# compute raw vs. centered\nbvm_r = braggpeaks.histogram( mode='raw', sampling=32 )\nbvm_c = braggpeaks.histogram( mode='cal', sampling=32 )\n\n# show\nshow( [bvm_r, bvm_c] ,vmax=0.999999)\n\n# show, zooming in on origin\nL = 60\nx,y = bvm_c.origin\nimport numpy as np\nx0,xf = np.round([x-L,x+L]).astype(int)\ny0,yf = np.round([y-L,y+L]).astype(int)\n\nshow(\n    [\n    bvm_r[x0:xf,y0:yf],\n    bvm_c[x0:xf,y0:yf]\n    ],\n    vmax=0.999999\n)\n\n","type":"content","url":"/strain#calibrate-the-origin","position":17},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl2":"Calibrate the rotation"},"type":"lvl2","url":"/strain#calibrate-the-rotation","position":18},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl2":"Calibrate the rotation"},"content":"\n\n# Compare a virtual image to the defocused CBED image\n\n#show(\n#   [im_bf,defocused_cbed]\n#)\n\n# # Find the QR rotation\n\nQR_rotation = 0   # in degrees\n\n# py4DSTEM.process.calibration.compare_QR_rotation(\n#     im_bf,\n#     defocused_cbed,\n#     QR_rotation,\n#     R_rotation = 158,\n#     R_position = (59,16.5),\n#     Q_position = (154,205),\n#     R_pos_anchor = 'tail',\n#     Q_pos_anchor = 'tail',\n#     R_length = 0.4,\n#     Q_length = 0.3,\n#     #figsize = (10,5),\n# )\n\n# Set the rotation\n\nbraggpeaks.calibration.set_QR_rotation_degrees( QR_rotation )\nbraggpeaks.calibration\n\n","type":"content","url":"/strain#calibrate-the-rotation","position":19},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl2":"Strain mapping"},"type":"lvl2","url":"/strain#strain-mapping","position":20},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl2":"Strain mapping"},"content":"\n\nstrainmap = py4DSTEM.StrainMap( braggvectors=braggpeaks )\n\nstrainmap.choose_basis_vectors(\n    minSpacing=4,\n    minAbsoluteIntensity=3e3,\n    maxNumPeaks=100,\n    edgeBoundary=1,\n    vis_params = {\n        'vmin' : 0,\n        'vmax' : 0.995\n    }\n)\n\nstrainmap.set_max_peak_spacing(\n    max_peak_spacing = 5\n)\n\nstrainmap.fit_basis_vectors(\n    max_peak_spacing = 3\n)\n\n# strain map\n\nstrainmap.get_strain(\n    gvects=None,\n    coordinate_rotation=0,\n    returncalc=False,\n)\n\nstrainmap.show_strain(\n    vrange=[-3, 3],\n    vrange_theta=[-3, 3],\n    vrange_exx=None,\n    vrange_exy=None,\n    vrange_eyy=None,\n    show_cbars=None,\n    bordercolor='k',\n    borderwidth=1,\n    titlesize=18,\n    ticklabelsize=10,\n    ticknumber=5,\n    unitlabelsize=16,\n    cmap='RdBu_r',\n    cmap_theta='PRGn',\n    mask_color='k',\n    color_axes='k',\n    show_legend=True,\n    show_gvects=True,\n    color_gvects='r',\n    legend_camera_length=1.6,\n    scale_gvects=0.6,\n    layout='square',\n    figsize=None,\n    returnfig=False,\n)\n\nstrainmap.show_reference_directions(\n    im_uncal=None,\n    im_cal=None,\n    color_axes='linen',\n    color_gvects='r',\n    origin_uncal=None,\n    origin_cal=None,\n    camera_length=1.8,\n    visp_uncal={'scaling': 'log'},\n    visp_cal={'scaling': 'log'},\n    layout='horizontal',\n    titlesize=16,\n    size_labels=14,\n    figsize=None,\n    returnfig=False,\n)\n\n","type":"content","url":"/strain#strain-mapping","position":21},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl2":"Strain using reference g_1 and g_2 from an ROI"},"type":"lvl2","url":"/strain#strain-using-reference-g-1-and-g-2-from-an-roi","position":22},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl2":"Strain using reference g_1 and g_2 from an ROI"},"content":"\n\n# Set reference region\n\nROI = np.zeros(braggpeaks.Rshape, dtype=bool)\nROI[84:112, 38:66] = True\n\nshow(\n    im_adf,\n    mask = ROI,\n    mask_color='r',\n    mask_alpha=0.6\n)\n\n# strain from a region\n\nstrainmap.get_strain(\n    gvects = ROI,\n    coordinate_rotation=0,\n    returncalc=False,\n)\n\n","type":"content","url":"/strain#strain-using-reference-g-1-and-g-2-from-an-roi","position":23},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl2":"Strain using a manually specified reference g_1 and g_2"},"type":"lvl2","url":"/strain#strain-using-a-manually-specified-reference-g-1-and-g-2","position":24},{"hierarchy":{"lvl1":"Intro to 4D-STEM data: strain","lvl2":"Strain using a manually specified reference g_1 and g_2"},"content":"\n\ng1_ref, g2_ref = strainmap.get_reference_g1g2( ROI )\n\nstrainmap.get_strain(\n    gvects=None,\n    coordinate_rotation=0,\n    returncalc=False\n)","type":"content","url":"/strain#strain-using-a-manually-specified-reference-g-1-and-g-2","position":25},{"hierarchy":{"lvl1":"Virtual imaging"},"type":"lvl1","url":"/orient","position":0},{"hierarchy":{"lvl1":"Virtual imaging"},"content":"import py4DSTEM\nimport time\nimport numpy as np\nfrom py4DSTEM.visualize import show\n\n#timestr = time.strftime(\"%Y%m%d\")\ntimestr = '20240814' # in case you want to use older data/not preprocess everything again\n\npy4DSTEM.__version__\n\n# Load the .dm4 file from a workspace on OMNI or XNAS, set filepath with 'filepath_data = \"FILEPATH\"'\ndirpath = \"/fast/ws-tmp/g031362-data/high_pressure_torsion/lamC/ROI3/\"\n\nfilepath_data = dirpath + 'COPL_Ni65Cu35_C_ROI3_240827_aper_50_conv_1.5_spot_6_CL_47_stepsize_10_r_x_178_r_y_186_GIF_512x512_preprocessed_unfiltered_no_bin.h5'\nfilepath_basics_01 = dirpath + 'analysis_basics_01.h5'\nfilepath_basics_02 = dirpath + 'analysis_basics_02.h5'\nfilepath_basics_03 = dirpath + 'analysis_basics_03.h5'\n\n\nfilepath_cif = dirpath + 'CuNi.cif'\n\npy4DSTEM.print_h5_tree(filepath_data)\n\n# Load the datacubes using py4DSTEM\ndataset = py4DSTEM.read(\n    filepath_data,\n    root='dm_dataset_root/dm_dataset',\n)\n\n","type":"content","url":"/orient","position":1},{"hierarchy":{"lvl1":"Virtual imaging"},"type":"lvl1","url":"/orient#virtual-imaging","position":2},{"hierarchy":{"lvl1":"Virtual imaging"},"content":"\n\ndataset.get_dp_max()\ndataset.get_dp_mean()\n\npy4DSTEM.visualize.show(\n    dataset.tree('dp_max'),\n    figsize = (4,4),\n    ticks = False,\n)\n\npy4DSTEM.show(\n    [\n        dataset.data[20,20],\n        dataset.data[60,20],\n        dataset.data[20,70],\n        dataset.data[50,45],\n        dataset.data[9,7],\n    ],\n    combine_images=True,\n    figsize = (4,4),\n)\n\n# Estimate the radius of the BF disk, and the center coordinates\nprobe_semiangle, probe_qx0, probe_qy0 = dataset.get_probe_size(\n    dataset.tree('dp_mean').data,\n)\ncenter = (probe_qx0, probe_qy0)\n\n# plot the mean diffraction pattern, with the estimated probe radius overlaid as a circle\nfig, ax = py4DSTEM.show(\n    dataset.tree('dp_mean'),\n    figsize=(4,4),\n    circle = {\n        'center': center,\n        'R': probe_semiangle,\n    },\n    ticks = False,\n    returnfig = True,\n    vmax = 1,\n);\n# ax.set_xlim([57, 70]);\n# ax.set_ylim([70, 57]);\n\n# Print the estimate probe radius\nprint('Estimated probe radius =', '%.2f' % probe_semiangle, 'pixels')\n\n# Create a virtual annular dark field (ADF) image around the first diffraction ring\n\nradii = (75,90)\n\n# Plot the ADF detector\ndataset.position_detector(\n    mode = 'annular',\n    geometry = (\n        center,\n        radii\n    ),\n    figsize = (4,4),\n    ticks = False,\n)\n\n# Calculate the ADF image\ndataset.get_virtual_image(\n    mode = 'annulus',\n    geometry = (center,radii),\n    name = 'dark_field',\n)\n\n# Plot the ADF image\npy4DSTEM.show(\n    dataset.tree('dark_field'),\n    figsize = (4,4),\n)\n\n","type":"content","url":"/orient#virtual-imaging","position":3},{"hierarchy":{"lvl1":"Probe template"},"type":"lvl1","url":"/orient#probe-template","position":4},{"hierarchy":{"lvl1":"Probe template"},"content":"\n\n# Load the prior analysis\n\nbasics_03 = py4DSTEM.read(\n    filepath_basics_03\n)\n\nbasics_03.tree()\n\nprobe = basics_03.tree('probe')\n\nprobe\n\npy4DSTEM.visualize.show_kernel(\n    probe.kernel,\n    R = 24,\n    L = 24,\n    W = 1\n)\n\n","type":"content","url":"/orient#probe-template","position":5},{"hierarchy":{"lvl1":"Bragg disk detection"},"type":"lvl1","url":"/orient#bragg-disk-detection","position":6},{"hierarchy":{"lvl1":"Bragg disk detection"},"content":"\n\n# Test parameters on a few probe positions\n# Visualize the diffraction patterns and the located disk positions\n\nrxs = 60,60,45\nrys = 60,40,36\ncolors=['r','limegreen','c']\n\n\ndetect_params = {\n    'minAbsoluteIntensity': 8,   # intensity threshold\n    'minRelativeIntensity': 0,   # int. thresh. relative to brightest disk in each pattern\n    'minPeakSpacing': 11,         # if two peaks are closer than this (in pixels), remove the dimmer peak\n    'edgeBoundary': 2,           # remove peaks within this distance of the edge of the diffraction pattern\n    'sigma': 0.4,                  # gaussian blur size to apply to cross correlation before finding maxima\n    'maxNumPeaks': 20,          # maximum number of peaks to return, in order of intensity\n    'subpixel' : 'poly',         # subpixel resolution method\n    'corrPower': 1.0,            # if <1.0, performs a hybrid cross/phase correlation. More sensitive to edges and to noise\n#     'CUDA': True,              # if a GPU is configured and cuda dependencies are installed, speeds up calculation \n}\n\n\n\ndisks_selected = dataset.find_Bragg_disks(\n    data = (rxs, rys),\n    template = probe.kernel,\n    **detect_params,\n)\n\npy4DSTEM.visualize.show_image_grid(\n    get_ar = lambda i:dataset.data[rxs[i],rys[i],:,:],\n    H=1, \n    W=3,\n    axsize=(3,3),\n    get_bordercolor = lambda i:colors[i],\n    get_x = lambda i: disks_selected[i].data['qx'],\n    get_y = lambda i: disks_selected[i].data['qy'],\n    get_pointcolors = lambda i: colors[i],\n    open_circles = True,\n    scale = 300,\n)\n\n# Find Bragg peaks for all probe positions\nbragg_peaks = dataset.find_Bragg_disks(\n    template = probe.kernel,\n    **detect_params,\n)\n\n","type":"content","url":"/orient#bragg-disk-detection","position":7},{"hierarchy":{"lvl1":"Centering and calibration"},"type":"lvl1","url":"/orient#centering-and-calibration","position":8},{"hierarchy":{"lvl1":"Centering and calibration"},"content":"\n\n# Compute the origin position for all probe positions by finding and then fitting the center beam\n\n# measure origins\nqxy_origins = bragg_peaks.measure_origin(\n    # mode = 'no_beamstop',\n)\n\n# fit a plane to the origins\nqx0_fit,qy0_fit,qx0_residuals,qy0_residuals = bragg_peaks.fit_origin(\n    # plot_range=0.1,\n    figsize = (4,4)\n)\n\n# Calculate BVM from centered data\nbragg_vector_map_centered = bragg_peaks.get_bvm()\n\npy4DSTEM.show(\n    bragg_vector_map_centered,\n    figsize = (4,4),\n)\n\n","type":"content","url":"/orient#centering-and-calibration","position":9},{"hierarchy":{"lvl1":"Pixel size calibration"},"type":"lvl1","url":"/orient#pixel-size-calibration","position":10},{"hierarchy":{"lvl1":"Pixel size calibration"},"content":"\n\n# Calculate and plot the radial integral.\n# Note that for a 2D material, the center beam is orders of magnitude higher than the diffracted beam.\n# Thus we need to specify a maximum y value for the plot.\n# We also scale the plotted intensity by q to better show the higher angle peaks.\nymax = 6e10\n\n\nq, intensity_radial = py4DSTEM.process.utils.radial_integral(\n    bragg_vector_map_centered,\n)\n\npy4DSTEM.visualize.show_qprofile(\n    q = q,\n    intensity = intensity_radial * q,\n    ymax = ymax,\n)\n\n# Load the WS2 crystal file\ncrystal = py4DSTEM.process.diffraction.Crystal.from_CIF(filepath_cif)\n\n# Calculate structure factors\nk_max = 1.4\n\ncrystal.calculate_structure_factors(\n    k_max,\n)\n\ncrystal.plot_structure(\n    zone_axis_lattice=(1,1,0.1),\n    figsize=(6,3),   \n    camera_dist = 6,\n)\n\n# Test different pixel sizes, and overlay the structure factors onto the experimental data.\n\ninv_Ang_per_pixel = 0.02\n\nq_SF = np.linspace(0,k_max,250)\nI_SF = np.zeros_like(q_SF)\nfor a0 in range(crystal.g_vec_leng.shape[0]):\n    if np.abs(crystal.g_vec_all[2,a0]) < 0.01:\n        idx = np.argmin(np.abs(q_SF-crystal.g_vec_leng[a0]))\n        I_SF[idx] += crystal.struct_factors_int[a0]\nI_SF /= np.max(I_SF)\n\nfig,ax = py4DSTEM.visualize.show_qprofile(\n    q=q*inv_Ang_per_pixel,\n    intensity=intensity_radial*q,\n    xlabel='q (1/Ang)',\n    returnfig=True,\n    ymax=ymax,\n)\n\nax.plot(q_SF,I_SF*ymax,c='r')\nax.set_xlim([0, k_max])\n\n# Apply pixel size calibration\nbragg_peaks.calibration.set_Q_pixel_size(inv_Ang_per_pixel)\nbragg_peaks.calibration.set_Q_pixel_units('A^-1')\n\nbragg_peaks.calstate\n\n# Save calibrated Bragg peaks\nfilepath_braggdisks_cal = dirpath + 'braggdisks_cal.h5'\npy4DSTEM.save(\n    filepath_braggdisks_cal,\n    bragg_peaks,\n    mode='o',\n)\n\n","type":"content","url":"/orient#pixel-size-calibration","position":11},{"hierarchy":{"lvl1":"Automated crystal orientation mapping (ACOM)"},"type":"lvl1","url":"/orient#automated-crystal-orientation-mapping-acom","position":12},{"hierarchy":{"lvl1":"Automated crystal orientation mapping (ACOM)"},"content":"\n\n# Reload Bragg peaks if needed\nfilepath_braggdisks_cal = dirpath + 'braggdisks_cal.h5'\npy4DSTEM.print_h5_tree(filepath_braggdisks_cal)\n\n# Reload bragg peaks cif file, recompute structure factors\nbragg_peaks = py4DSTEM.read(\n    filepath_braggdisks_cal, \n)\nbragg_peaks\n\nk_max = 1.4\ncrystal = py4DSTEM.process.diffraction.Crystal.from_CIF(filepath_cif)\ncrystal.calculate_structure_factors(\n    k_max,\n)\n\n# Create an orientation plan for [0001] WS2\ncrystal.orientation_plan(\n    angle_step_zone_axis = 1.0,\n    angle_step_in_plane = 4.0,\n    zone_axis_range = 'fiber',\n    fiber_axis = [0,0,1],\n    fiber_angles = [0,0],\n#     CUDA=True,\n)\n\n# Test matching on some probe positions\nxind, yind = 20,64\n#xind, yind= 32,96\n\norientation  = crystal.match_single_pattern(\n    bragg_peaks.cal[xind,yind],\n#     plot_corr = True,\n#     plot_polar = False,\n    verbose = True,\n)\n\nsigma_compare = 0.03\nrange_plot = np.array([k_max+0.1,k_max+0.1])\n\nbragg_peaks_fit = crystal.generate_diffraction_pattern(\n    orientation,\n    ind_orientation=0,\n    sigma_excitation_error=sigma_compare)\n\n\n# plot comparisons\npy4DSTEM.process.diffraction.plot_diffraction_pattern(\n    bragg_peaks_fit,\n    bragg_peaks_compare=bragg_peaks.cal[xind,yind],\n    scale_markers=1000,\n    scale_markers_compare=4e4,\n    plot_range_kx_ky=range_plot,\n    min_marker_size=1,\n    figsize = (5,5),\n)\n\n# Fit orientation to all probe positions\norientation_map = crystal.match_orientations(\n    bragg_peaks,\n)\n\n# Plot the orienations \nimages_orientation = crystal.plot_fiber_orientation_maps(\n    orientation_map,\n    symmetry_order = 6,\n    corr_range = [0.9, 1.0],\n    figsize = (4,4),\n)\n\n","type":"content","url":"/orient#automated-crystal-orientation-mapping-acom","position":13},{"hierarchy":{"lvl1":"Strain maps"},"type":"lvl1","url":"/orient#strain-maps","position":14},{"hierarchy":{"lvl1":"Strain maps"},"content":"For each diffraction pattern, we have both the measured diffraction pattern Bragg peaks, and the Bragg peaks calculated from the best-fit orientation. We can therefore directly calculate a strain map, just by measuring the best-fit transformation tensor between these two sets of peaks.\n\nstrain_map = crystal.calculate_strain(\n    bragg_peaks,\n    orientation_map,\n    rotation_range=np.pi/3, # 60 degrees\n#     corr_kernel_size=0.02,\n)\n\n# plot the 4 components of the strain tensor\nfig,ax = py4DSTEM.visualize.show_strain(\n    strain_map,\n    vrange_exx=[-3.0, 3.0],\n    vrange_theta=[0.0, 60.0],\n    ticknumber=3,\n    # axes_plots=(),\n    # bkgrd=False,\n    figsize=(6,6),\n    cmap='hsv',\n    returnfig=True\n)","type":"content","url":"/orient#strain-maps","position":15},{"hierarchy":{"lvl1":"Load datacube"},"type":"lvl1","url":"/acom","position":0},{"hierarchy":{"lvl1":"Load datacube"},"content":"# timestr used to create simple timestamps for easier version controll\nimport time\ntimestr = time.strftime(\"%Y%m%d\")\n\n# py4dstem as main tool, the follwing command also prints the currently used version\nimport py4DSTEM\nfrom py4DSTEM import show\npy4DSTEM.__version__\n\n# Load the .dm4 file from a workspace on OMNI or XNAS, set filepath with 'filepath_data = \"FILEPATH\"'\n# even better, use dirpath'' and 'filepath_data', this makes it easier to add more paths later\n\ndirpath = \"/Users/paullobpreis/GitHub/Paullo9.github.io/data/\"\n\nfilepath_data = dirpath + 'COPL_Ni65Cu35_C_ROI3_240827_aper_50_conv_1.5_spot_6_CL_47_stepsize_10_r_x_178_r_y_186_GIF_512x512_preprocessed_unfiltered_bin_4_20240828.h5'\nfilepath_cif = dirpath + 'CuNi.cif'\n\n\n# Load the datacubes using py4DSTEM\n\ndatacube = py4DSTEM.read(\n    filepath = filepath_data,\n    datapath = 'dm_dataset_root/dm_dataset'\n)\n\ndatacube\n\n","type":"content","url":"/acom","position":1},{"hierarchy":{"lvl1":"Load datacube","lvl2":"Virtual imaging"},"type":"lvl2","url":"/acom#virtual-imaging","position":2},{"hierarchy":{"lvl1":"Load datacube","lvl2":"Virtual imaging"},"content":"\n\n# First we need to position the detector\n# we can set the geometry by hand\n\ncenter = (64,64)\n\nradius = 10\n\n\n# overlay selected detector position over mean dp\ndatacube.position_detector(\n    mode = 'circle',\n    geometry = (\n        center,\n        radius\n    )\n)\n\n# Capture/compute the virtual BF \n\ndatacube.get_virtual_image(\n    mode = 'circle',\n    geometry = (center,radius),\n    name = 'bright_field',       # the output will be stored in `datacube`'s tree with this name\n)\n\n# and show the result\nfig, ax = show( datacube.tree('bright_field'),\n    returnfig = True,\n        cmap='viridis'\n              )\n\n\n","type":"content","url":"/acom#virtual-imaging","position":3},{"hierarchy":{"lvl1":"Load datacube","lvl2":"Synthetic probe template"},"type":"lvl2","url":"/acom#synthetic-probe-template","position":4},{"hierarchy":{"lvl1":"Load datacube","lvl2":"Synthetic probe template"},"content":"\n\n# define the semiangle\nprobe_semiangle = 0.75\n\n# generate probe\nprobe = py4DSTEM.Probe.generate_synthetic_probe(\n    radius = probe_semiangle,\n    width = 0.7,\n    Qshape = datacube.Qshape\n)\n\nprobe.get_kernel(\n    mode = 'sigmoid',\n    origin = (datacube.Qshape[0]/2,datacube.Qshape[0]/2),\n    radii = (probe_semiangle * 1.2,\n             probe_semiangle * 4\n    )\n)\n\npy4DSTEM.visualize.show_kernel(\n    probe.kernel,\n    R = 15,\n    L = 15,\n    W = 1\n)\n\n","type":"content","url":"/acom#synthetic-probe-template","position":5},{"hierarchy":{"lvl1":"Load datacube","lvl2":"Bragg disk detection"},"type":"lvl2","url":"/acom#bragg-disk-detection","position":6},{"hierarchy":{"lvl1":"Load datacube","lvl2":"Bragg disk detection"},"content":"\n\n# Select a few sample patterns for parameter tuning\n\nrxs = 25,150,80,105,50,58\nrys = 15,90,141,60,90,27\n\ncolors=['r','limegreen','c','g','orange', 'violet']\n\n\n# visualize\n\ncolors=['r','limegreen','c','g','orange', 'violet']\n\n# show the selected\n# positions in real space\nfig, ax = py4DSTEM.visualize.show_points(\n    datacube.tree('bright_field'),\n    x=rxs,\n    y=rys,\n    scale=300,\n    pointcolor=colors,\n    figsize=(8,8),\n    returnfig = True\n)\n\n\n# show the selected\n# diffraction patterns\nfig, ax = py4DSTEM.visualize.show_image_grid(\n    get_ar = lambda i:datacube[rxs[i],rys[i],:,:],\n    H=2,W=3,\n    axsize=(5,5),\n    intensity_range='absolute',\n    vmin=2e4,\n    vmax=1e5,\n    scaling='power',\n    power=0.75,\n    get_bordercolor = lambda i:colors[i],\n    returnfig = True\n)\n\n\n# Choose disk detection parameters\n\n# set parameters\n\ndetect_params = {\n    'minAbsoluteIntensity': 1e4,   # intensity threshold\n    'minRelativeIntensity': 0,   # int. thresh. relative to brightest disk in each pattern\n    'minPeakSpacing': 10,         # if two peaks are closer than this (in pixels), remove the dimmer peak\n    'edgeBoundary': 10,           # remove peaks within this distance of the edge of the diffraction pattern\n    'sigma': 0.4,                  # gaussian blur size to apply to cross correlation before finding maxima\n    'maxNumPeaks': 1000,          # maximum number of peaks to return, in order of intensity\n    'subpixel' : 'poly',         # subpixel resolution method\n    'corrPower': 1,            # if <1.0, performs a hybrid cross/phase correlation. More sensitive to edges and to noise\n#     'CUDA': True,              # if a GPU is configured and cuda dependencies are installed, speeds up calculation \n}\n\n\n# find disks for selected patterns\ndisks_selected = datacube.find_Bragg_disks(\n    data = (rxs, rys),\n    template = probe.kernel,\n    **detect_params,\n)\n\n# show\nfig, ax = py4DSTEM.visualize.show_image_grid(\n    get_ar = lambda i:datacube[rxs[i],rys[i],:,:],\n    H=2, \n    W=3,\n    axsize=(5,5),\n    intensity_range='absolute',\n    scaling='power',\n    power=0.5,\n    get_bordercolor = lambda i:colors[i],\n    get_x = lambda i: disks_selected[i].data['qx'],\n    get_y = lambda i: disks_selected[i].data['qy'],\n    get_pointcolors = lambda i: colors[i],\n    open_circles = True,\n    scale = 650,\n    returnfig = True,\n    vmin = 0,\n    vmax = 1e5,\n)\n\n\n# Find Bragg peaks for all probe positions\nbragg_peaks = datacube.find_Bragg_disks(\n    template = probe.kernel,\n    **detect_params,\n)\n\nLocalizing the disks measures the Bragg points.  Just like the max and mean diffraction patterns allow us to get a succinct but highly informative picture of the whole dataset all at once, it‚Äôs useful to condense all the information about Bragg scattering we just measured into a single visualization.  We do this with a Bragg vector map - a 2D histogram of all the Bragg vector positions and intensities we‚Äôve measured.\n\n# Bragg vector map\n\n# compute\nbvm = bragg_peaks.histogram( mode='raw' )\n\n# show\nshow(bvm)\n\n# Increasing the sampling can be helpful to visualize the resolution and error of our measurement.\n\n# compute\nbvm_upsampled = bragg_peaks.histogram(\n    mode='raw',\n    sampling = 8\n)\n\n# show\nshow(bvm_upsampled)\n\n# set a filepath\n\nfilepath_save = dirpath + 'ACOM_HPT_ROI3_braggvectors.h5'\nfilepath_save\n\n# inspect what's in `datacube`'s  tree\n\ndatacube.tree()\n\n# save everthing except the datacube\n\npy4DSTEM.save(\n    filepath_save,\n    datacube,\n    tree = None,  # everything *under* datacube, but not not datacube itself\n    mode = 'o'\n)\n\n# inspect the resulting HDF5 file\n\npy4DSTEM.print_h5_tree(filepath_save)\n\n","type":"content","url":"/acom#bragg-disk-detection","position":7},{"hierarchy":{"lvl1":"Load datacube","lvl2":"Centering and calibration"},"type":"lvl2","url":"/acom#centering-and-calibration","position":8},{"hierarchy":{"lvl1":"Load datacube","lvl2":"Centering and calibration"},"content":"\n\n","type":"content","url":"/acom#centering-and-calibration","position":9},{"hierarchy":{"lvl1":"Load datacube","lvl3":"calibrate the origin","lvl2":"Centering and calibration"},"type":"lvl3","url":"/acom#calibrate-the-origin","position":10},{"hierarchy":{"lvl1":"Load datacube","lvl3":"calibrate the origin","lvl2":"Centering and calibration"},"content":"\n\n# All the calibrations are still set to False because we haven't performed\n# any calibration measurements yet!\n\nbragg_peaks.calstate\n\nbragg_peaks.calibration\n\n# Let's confirm that the raw vectors represent the positions of\n# detected bragg disks in a sample diffraction pattern\n\ndp = datacube[163,163]\nv = bragg_peaks.raw[63,63]\n\nshow(\n    dp,\n    points = {\n        'x' : v.qx,\n        'y' : v.qy,\n    }\n)\n\n# Measure the origin\n\nqx0_meas,qy0_meas,mask_meas = bragg_peaks.measure_origin()\n\nshow(\n    [qx0_meas,qy0_meas],\n    cmap = 'viridis',\n    mask = mask_meas\n)\n\n# Fit a plane to the origins\n\nqx0_fit,qy0_fit,qx0_residuals,qy0_residuals = bragg_peaks.fit_origin()\n\n# Now that we've calibrated the center positions, we can re-compute\n# the Bragg vector map, this time with the center correction applied\n\nsampling = 8\n\n# compute\nbvm = bragg_peaks.histogram(\n    #mode='cal',             # 'cal' is the default mode, so this line can be included or left out\n    sampling = sampling,\n)\n\n# show\n# overlay a circle around the center for visualization purposes\nshow(\n    bvm,\n    circle={\n        'center' : bvm.origin,   # the centered BVM knows where its origin is \n        'R' : 4*sampling,\n        'fill' : False,\n        'linewidth' : 1\n    },\n    #vmax=0.9\n)\n\n# Compare this to the uncalibrated BVM - much better!\n\n# compute raw vs. centered\nbvm_r = bragg_peaks.histogram( mode='raw', sampling=sampling )\nbvm_c = bragg_peaks.histogram( mode='cal', sampling=sampling )\n\n# show\nshow( [bvm_r, bvm_c] ,vmax=1e5)\n\n# show, zooming in on origin\nL = 50\nx,y = bvm_c.origin\nimport numpy as np\nx0,xf = np.round([x-L,x+L]).astype(int)\ny0,yf = np.round([y-L,y+L]).astype(int)\n\nshow(\n    [\n    bvm_r[x0:xf,y0:yf],\n    bvm_c[x0:xf,y0:yf]\n    ],\n    vmax=1e5\n)\n\n","type":"content","url":"/acom#calibrate-the-origin","position":11},{"hierarchy":{"lvl1":"Load datacube","lvl3":"calibrate the ellipticity","lvl2":"Centering and calibration"},"type":"lvl3","url":"/acom#calibrate-the-ellipticity","position":12},{"hierarchy":{"lvl1":"Load datacube","lvl3":"calibrate the ellipticity","lvl2":"Centering and calibration"},"content":"\n\n# Select an annular region in which to perform a fit\n# The ideal is a single, isolated ring of peaks\n\nq_range = (120, 200)\n\npy4DSTEM.show(\n    bvm_c,\n    cmap='gray',\n    intensity_range='absolute',\n    vmin=0,\n    vmax=1e5,\n    annulus={\n        'center':bvm_c.origin,\n        'radii': q_range,'fill':True,'color':'r','alpha':0.3}\n)\n\n# Fit the elliptical distortions\np_ellipse = py4DSTEM.process.calibration.fit_ellipse_1D(\n    bvm_c,\n    center = bvm_c.origin,\n    fitradii = q_range,\n)\n\n# plot the fit\npy4DSTEM.visualize.show_elliptical_fit(\n    bvm_c,\n    q_range,\n    p_ellipse,\n    cmap='gray',\n    intensity_range='absolute',\n    vmin=0,\n    vmax=1e5,\n)\n\np_ellipse\n\n# The elliptical parameters are not automatically added to the calibration metadata,\n# (to allow inspection of the fit to ensure it's accurate), so need to be added manually\n# once a good fit is found. Like so:\n\nbragg_peaks.calibration.set_p_ellipse(p_ellipse)\n\n# Note that the code above only adds (a,b,theta) to the calibration metadata; the origin needs to\n# be calibrated separately, as we did above \n\nbragg_peaks.calibration\n\n# Calibrate, compute a new bragg vector map, and compare\n\nbragg_peaks.setcal()\nbvm_e = bragg_peaks.histogram(\n    sampling=sampling\n)\n\nshow([bvm_e, bvm_r],vmax=0.99)\n\n","type":"content","url":"/acom#calibrate-the-ellipticity","position":13},{"hierarchy":{"lvl1":"Load datacube","lvl2":"Calibrate the detector pixel size"},"type":"lvl2","url":"/acom#calibrate-the-detector-pixel-size","position":14},{"hierarchy":{"lvl1":"Load datacube","lvl2":"Calibrate the detector pixel size"},"content":"\n\nimport numpy as np\nfrom py4DSTEM.process.diffraction import Crystal\n\n# Define lattice parameter, atomic numbers, and k_max for structure factor calculation\na_lat = 3.55  # approximate lattice parameter for Cu-Ni alloy in angstroms\natom_num = 29  # atomic numbers for Cu (29) and Ni (28)\nk_max = 2  # maximum k value for structure factor calculation\n\n# Example: Alternating Cu and Ni in FCC positions\npos = np.array([\n    [0.0, 0.0, 0.0],  # Cu\n    [0.0, 0.5, 0.5],  # Ni\n    [0.5, 0.0, 0.5],  # Cu\n    [0.5, 0.5, 0.0],  # Ni\n])\n\n# Make crystal\ncrystal = py4DSTEM.process.diffraction.Crystal(\n    pos, \n    atom_num, \n    a_lat)\n# Calculate structure factors\ncrystal.calculate_structure_factors(k_max)\n\n\n\n# Make an initial guess at the pixel size to refine\n# Let's estimate eith an overlay of the measured scattering and reference crystal structure.\n\n\n# Modify `pixel_size_inv_Ang_guess` until it \n# looks close before attempting to fit the data!\npixel_size_inv_Ang_guess = 0.0275\n\n\n# calibrate\nbragg_peaks.calibration.set_Q_pixel_size(pixel_size_inv_Ang_guess)\nbragg_peaks.calibration.set_Q_pixel_units('A^-1')\nbragg_peaks.setcal()\n\n# show overlay\ncrystal.plot_scattering_intensity(\n    bragg_peaks = bragg_peaks,\n    bragg_k_power = 2.0\n)\n\n# fit pixel size to lattice\n\ncrystal.calibrate_pixel_size(\n    bragg_peaks = bragg_peaks,\n    bragg_k_power = 2.0,\n    plot_result = True,\n);\n\nbragg_peaks.calibration\n\n# New bvm, compare\n\nbragg_peaks.setcal()\nbvm_p = bragg_peaks.histogram(\n    sampling=sampling\n)\n\nshow([bvm_p, bvm_r],vmax=0.99)\n\nbvm_p\n\n","type":"content","url":"/acom#calibrate-the-detector-pixel-size","position":15},{"hierarchy":{"lvl1":"Load datacube","lvl3":"calibrate the rotation","lvl2":"Calibrate the detector pixel size"},"type":"lvl3","url":"/acom#calibrate-the-rotation","position":16},{"hierarchy":{"lvl1":"Load datacube","lvl3":"calibrate the rotation","lvl2":"Calibrate the detector pixel size"},"content":"\n\nQR_rotation = 0\n\n# Set the rotation\n\nbragg_peaks.calibration.set_QR_rotation_degrees( QR_rotation )\nbragg_peaks.calibration\n\n","type":"content","url":"/acom#calibrate-the-rotation","position":17},{"hierarchy":{"lvl1":"Load datacube","lvl2":"Strain mapping"},"type":"lvl2","url":"/acom#strain-mapping","position":18},{"hierarchy":{"lvl1":"Load datacube","lvl2":"Strain mapping"},"content":"\n\nstrainmap = py4DSTEM.StrainMap( braggvectors=bragg_peaks )\n\nstrainmap.choose_basis_vectors(\n    minSpacing=10,\n    minAbsoluteIntensity=3e3,\n    maxNumPeaks=20,\n    edgeBoundary=40,\n    vis_params = {\n        'vmin' : 0,\n        'vmax' : 0.995\n    }\n)\n\nstrainmap.set_max_peak_spacing(\n    max_peak_spacing = 5\n)\n\nstrainmap.fit_basis_vectors(\n    max_peak_spacing = 3\n)\n\n# strain map\n\nstrainmap.get_strain(\n    gvects=None,\n    coordinate_rotation=0,\n    returncalc=False,\n)\n\nstrainmap.show_strain(\n    vrange=[-3, 3],\n    vrange_theta=[-3, 3],\n    vrange_exx=None,\n    vrange_exy=None,\n    vrange_eyy=None,\n    show_cbars=None,\n    bordercolor='k',\n    borderwidth=1,\n    titlesize=18,\n    ticklabelsize=10,\n    ticknumber=5,\n    unitlabelsize=16,\n    cmap='RdBu_r',\n    cmap_theta='PRGn',\n    mask_color='k',\n    color_axes='k',\n    show_legend=True,\n    show_gvects=True,\n    color_gvects='r',\n    legend_camera_length=1.6,\n    scale_gvects=0.6,\n    layout='square',\n    figsize=None,\n    returnfig=False,\n)\n\nstrainmap.show_reference_directions(\n    im_uncal=None,\n    im_cal=None,\n    color_axes='linen',\n    color_gvects='r',\n    origin_uncal=None,\n    origin_cal=None,\n    camera_length=1.8,\n    visp_uncal={'scaling': 'log'},\n    visp_cal={'scaling': 'log'},\n    layout='horizontal',\n    titlesize=16,\n    size_labels=14,\n    figsize=None,\n    returnfig=False,\n)\n\n","type":"content","url":"/acom#strain-mapping","position":19},{"hierarchy":{"lvl1":"Load datacube","lvl2":"Pixel size calibration"},"type":"lvl2","url":"/acom#pixel-size-calibration","position":20},{"hierarchy":{"lvl1":"Load datacube","lvl2":"Pixel size calibration"},"content":"\n\n#Load the cif crystal file\nfrom py4DSTEM import Crystal\n\ncrystal = py4DSTEM.process.diffraction.Crystal.from_CIF(filepath_cif)\nprint(crystal.lat_real)\n\n# Calculate structure factors\nstructure_factors, intensities = crystal.calculate_structure_factors(\n    k_max=k_max,\n    return_intensities=True\n)\n\n# Print the results\nprint(\"Structure Factors:\", structure_factors)\nprint(\"Intensities:\", intensities)\n\n\n# Compare measured diffraction pattern with reference crystal structure\ncrystal.plot_scattering_intensity(\n    bragg_peaks = bragg_peaks,\n    bragg_k_power = 2.0,\n)\n\n# Display the current pixel size\nprint('   Initial pixel size = ' + \\\n    str(np.round(bragg_peaks.calibration.get_Q_pixel_size(),8)) + \\\n    ' ' + bragg_peaks.calibration.get_Q_pixel_units())\n\n# Calibrate pixel size\nbragg_peaks_cali = crystal.calibrate_pixel_size(\n    bragg_peaks = bragg_peaks,\n    bragg_k_power = 2.0,\n    plot_result = True,\n    figsize = (8,3),\n)\n\n# Save calibrated Bragg peaks\nfilepath_braggdisks_cal = dirpath + 'braggdisks_cal_ROI3.h5'\n\npy4DSTEM.save(\n    filepath_braggdisks_cal,\n    bragg_peaks_cali,\n    mode='o',\n)\n\n","type":"content","url":"/acom#pixel-size-calibration","position":21},{"hierarchy":{"lvl1":"Load datacube","lvl2":"Automated crystal orientation mapping (ACOM)"},"type":"lvl2","url":"/acom#automated-crystal-orientation-mapping-acom","position":22},{"hierarchy":{"lvl1":"Load datacube","lvl2":"Automated crystal orientation mapping (ACOM)"},"content":"\n\n# Reload Bragg peaks if needed\nfilepath_braggdisks_cal = dirpath + 'braggdisks_cal_ROI3.h5'\npy4DSTEM.print_h5_tree(filepath_braggdisks_cal)\n\n# Reload bragg peaks cif file, recompute structure factors\nbragg_peaks = py4DSTEM.read(\n    filepath_braggdisks_cal, \n)\nbragg_peaks\n\nk_max = 1.4\ncrystal = py4DSTEM.process.diffraction.Crystal.from_CIF(filepath_cif)\ncrystal.calculate_structure_factors(\n    k_max,\n)\n\n\n# Create an orientation plan for [001] \ncrystal.orientation_plan(\n    angle_step_zone_axis = 1.0,\n    angle_step_in_plane = 4.0,\n    zone_axis_range = 'fiber',\n    fiber_axis = [0,0,1],\n    fiber_angles = [0,0],\n#     CUDA=True,\n)\n\nimport numpy as np\n\n# Test matching on some probe positions\nxind, yind = 20,64\n#xind, yind= 33,25\n\norientation  = crystal.match_single_pattern(\n    bragg_peaks.cal[xind,yind],\n#     plot_corr = True,\n#     plot_polar = False,\n    verbose = True,\n)\n\nsigma_compare = 0.03\nrange_plot = np.array([k_max+0.1,k_max+0.1])\n\nbragg_peaks_fit = crystal.generate_diffraction_pattern(\n    orientation,\n    ind_orientation=0,\n    sigma_excitation_error=sigma_compare)\n\n\n# plot comparisons\npy4DSTEM.process.diffraction.plot_diffraction_pattern(\n    bragg_peaks_fit,\n    bragg_peaks_compare=bragg_peaks.cal[xind,yind],\n    scale_markers=1000,\n    scale_markers_compare=4e4,\n    plot_range_kx_ky=range_plot,\n    min_marker_size=1,\n    figsize = (5,5),\n)\n\n# Fit orientation to all probe positions\norientation_map = crystal.match_orientations(\n    bragg_peaks,\n)\n\norientation_map.angles[0:2,0:2,0,2]\n\n# Plot the orienations \nimages_orientation = crystal.plot_fiber_orientation_maps(\n    orientation_map,\n    symmetry_order = 4,\n    corr_range = [0,3], #Correlation intensity range for the plot\n    dir_in_plane_degrees = 0.0, #Reference in-plane angle (degrees). Default is 0 / x-axis / vertical down.\n    corr_normalize = True, # If true, set mean correlation to 1.\n    cmap_out_of_plane= 'plasma', \n    figsize = (12, 8),\n)\n\n","type":"content","url":"/acom#automated-crystal-orientation-mapping-acom","position":23},{"hierarchy":{"lvl1":"Load datacube","lvl2":"In/Out of plane orientation mapping"},"type":"lvl2","url":"/acom#in-out-of-plane-orientation-mapping","position":24},{"hierarchy":{"lvl1":"Load datacube","lvl2":"In/Out of plane orientation mapping"},"content":"\n\n# Reload Bragg peaks if needed\n#filepath_braggdisks_cal = dirpath + 'braggdisks_cal_ROI3.h5'\n#py4DSTEM.print_h5_tree(filepath_braggdisks_cal)\n\n\n# Example: Alternating Cu and Ni in FCC positions\npos = np.array([\n    [0.0, 0.0, 0.0],  # Cu\n    [0.0, 0.5, 0.5],  # Ni\n    [0.5, 0.0, 0.5],  # Cu\n    [0.5, 0.5, 0.0],  # Ni\n])\n\natom_num = 29\na = 3.55\ncell = a\n\ncrystal = py4DSTEM.process.diffraction.Crystal(\n    pos, \n    atom_num, \n    cell)\n\n\n# Calculate and plot structure factors\nk_max = 1.5\n\ncrystal.calculate_structure_factors(\n    k_max,\n)\n\ncrystal.plot_structure_factors(\n    zone_axis_lattice=[1,1,1],\n    figsize = (4,4),\n)\n\ncrystal.plot_scattering_intensity(\n    bragg_peaks = bragg_peaks_cali,\n    bragg_k_power = 2.0,\n    figsize = (8,2),\n)\n\n# Make and plot a virtual dark field image to find the non-vacuum probe positions\nim_DF = bragg_peaks_cali.get_virtual_image(\n    mode = 'annular',\n    geometry=((),(0.3,0.6)),\n)\n\npy4DSTEM.show(\n    im_DF,\n    cmap='turbo',\n)\n\n# # Create an orientation plan using pymatgen\n\ncrystal.orientation_plan(\n    zone_axis_range = 'auto',\n    angle_step_zone_axis = 2.0,\n    #\n    # angle_step_zone_axis = 0.5,\n    # angle_coarse_zone_axis = 2.0,\n    # angle_refine_range = 2.0,\n    #\n    angle_step_in_plane = 5.0,\n    accel_voltage = 200e3,\n#     CUDA = True,\n#     intensity_power = 0.5,\n#     intensity_power = 0.125,\n#     radial_power = 2.0,\n    # corr_kernel_size = 0.12,\n    # tol_peak_delete = 0.08,\n)\n\nimport matplotlib.pyplot as plt\n\n# Test matching on some probe positions\nsigma_compare = 0.03  # Excitation error for the simulated diffraction patterns\nfigsize = (12,4)\n\nxind, yind = 34, 12\nxind, yind = 40, 93\n# xind, yind = 30, 40\n\n# plotting parameters\nplot_params = {\n    'scale_markers': 500,\n    'scale_markers_compare': 10,\n    'plot_range_kx_ky': crystal.k_max,\n    'min_marker_size': 2,\n}\n\n# Find best fit orientations\norientation  = crystal.match_single_pattern(\n    bragg_peaks_cali.cal[xind,yind],\n    num_matches_return = 3,\n    verbose = True,\n)\n\n# Simulated bragg peaks from best fit orientations\npeaks_fit_0 = crystal.generate_diffraction_pattern(\n    orientation,\n    ind_orientation=0,\n    sigma_excitation_error=sigma_compare)\npeaks_fit_1 = crystal.generate_diffraction_pattern(\n    orientation,\n    ind_orientation=1,\n    sigma_excitation_error=sigma_compare)\npeaks_fit_2 = crystal.generate_diffraction_pattern(\n    orientation,\n    ind_orientation=2,\n    sigma_excitation_error=sigma_compare)\n\n# plot comparisons\nfig,ax = plt.subplots(1,3,figsize=figsize)\n\npy4DSTEM.process.diffraction.plot_diffraction_pattern(\n    peaks_fit_0,\n    bragg_peaks_compare=bragg_peaks_cali.cal[xind,yind],\n    **plot_params,\n    input_fig_handle=(fig,[ax[0]]),\n)\npy4DSTEM.process.diffraction.plot_diffraction_pattern(\n    peaks_fit_1,\n    bragg_peaks_compare=bragg_peaks_cali.cal[xind,yind],\n    **plot_params,\n    input_fig_handle=(fig,[ax[1]]),\n)\npy4DSTEM.process.diffraction.plot_diffraction_pattern(\n    peaks_fit_2,\n    bragg_peaks_compare=bragg_peaks_cali.cal[xind,yind],\n    **plot_params,\n    input_fig_handle=(fig,[ax[2]]),\n)\n\n# Fit orientation to all probe positions\norientation_map = crystal.match_orientations(\n    bragg_peaks_cali,\n    num_matches_return = 1,\n    min_number_peaks = 3,\n)\n\n# plot orientation map\nimages_orientation = crystal.plot_orientation_maps(\n    orientation_map,\n    orientation_ind=0,\n    corr_range = np.array([1,4]),\n    camera_dist = 10,\n    show_axes = False,\n)\n\n# set a filepath\n\nACOM_ROI3 = dirpath + 'ACOM_HPT_ROI3.h5'\n\nACOM_ROI3\n\n# inspect what's in the data tree\n\ndatacube.tree()\n\n# save\npy4DSTEM.save(\n    ACOM_ROI3,\n    datacube,\n    tree=None,\n    mode = 'o'\n)\n\n# inspect the resulting HDF5 file\n\npy4DSTEM.print_h5_tree(ACOM_ROI3)","type":"content","url":"/acom#in-out-of-plane-orientation-mapping","position":25}]}